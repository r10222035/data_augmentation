{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 10:50:53.531056: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 10:50:53.620341: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# solve the problem of \"libdevice not found at ./libdevice.10.bc\"\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/home/r10222035/.conda/envs/tf2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path):\n",
    "    # path: run path\n",
    "    name = os.path.split(path)[1]\n",
    "\n",
    "    with open(os.path.join(path, f'{name}_tag_1_banner.txt')) as f:\n",
    "        for line in f.readlines():\n",
    "\n",
    "            #  Integrated weight (pb)  :       0.020257\n",
    "            match = re.match('#  Integrated weight \\(pb\\)  : +(\\d+\\.\\d+)', line)\n",
    "            if match:\n",
    "                # unit: fb\n",
    "                cross_section = float(match.group(1)) * 1000\n",
    "            #  Number of Events        :       100000\n",
    "            match = re.match('#  Number of Events        :       (\\d+)', line)\n",
    "            if match:\n",
    "                # unit: fb\n",
    "                nevent = int(match.group(1))\n",
    "\n",
    "    return cross_section, nevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nevent_in_SR_SB(sensitivity=1.0):\n",
    "    results_s = np.load('../Sample/HVmodel/data/selection_results_SB_4400_5800_s.npy', allow_pickle=True).item()\n",
    "    results_b = np.load('../Sample/HVmodel/data/selection_results_SB_4400_5800_b.npy', allow_pickle=True).item()\n",
    "\n",
    "    # Total cross section and number of events\n",
    "    xection, _ = get_info('../Sample/ppjj/Events/run_03')\n",
    "\n",
    "    # cross section in signal region and sideband region\n",
    "    cross_section_SR = results_b['cutflow_number']['Signal region'] / results_b['cutflow_number']['Total'] * xection\n",
    "    cross_section_SB = results_b['cutflow_number']['Sideband region'] / results_b['cutflow_number']['Total'] * xection\n",
    "    print(f'Background cross section, SR: {cross_section_SR:.2f} fb, SB: {cross_section_SB:.2f} fb')\n",
    "\n",
    "    # number of background events in signal region and sideband region\n",
    "    L = 139 * 1\n",
    "    n_SR_B = cross_section_SR * L\n",
    "    n_SB_B = cross_section_SB * L\n",
    "\n",
    "    print(f'Background sample size: SR: {n_SR_B:.1f}, SB: {n_SB_B:.1f}')\n",
    "\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "    print(f'Signal sample size: SR: {n_SR_S:.1f}, SB: {n_SB_S:.1f}')\n",
    "\n",
    "    return n_SR_S, n_SR_B, n_SB_S, n_SB_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SR_SB_sample_from(npy_dirs: list, nevents: tuple, seed=0):\n",
    "    # npy_dirs: list of npy directories\n",
    "    # nevents: tuple of (n_sig_SR, n_sig_SB, n_bkg_SR, n_bkg_SB)\n",
    "    data_SR = None\n",
    "    label_SR = None\n",
    "\n",
    "    data_SB = None\n",
    "    label_SB = None\n",
    "\n",
    "    data_sig_SR = np.load(os.path.join(npy_dirs[0], 'sig_in_SR-data.npy'))\n",
    "    data_sig_SB = np.load(os.path.join(npy_dirs[0], 'sig_in_SB-data.npy'))\n",
    "    data_bkg_SR = np.load(os.path.join(npy_dirs[0], 'bkg_in_SR-data.npy'))\n",
    "    data_bkg_SB = np.load(os.path.join(npy_dirs[0], 'bkg_in_SB-data.npy'))\n",
    "\n",
    "    n_sig_SR, n_sig_SB, n_bkg_SR, n_bkg_SB = nevents\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    idx_sig_SR = np.random.choice(data_sig_SR.shape[0], n_sig_SR, replace=False)\n",
    "    idx_sig_SB = np.random.choice(data_sig_SB.shape[0], n_sig_SB, replace=False)\n",
    "    idx_bkg_SR = np.random.choice(data_bkg_SR.shape[0], n_bkg_SR, replace=False)\n",
    "    idx_bkg_SB = np.random.choice(data_bkg_SB.shape[0], n_bkg_SB, replace=False)\n",
    "\n",
    "    print(f'Preparing dataset from {npy_dirs}')\n",
    "    for npy_dir in npy_dirs:\n",
    "\n",
    "        data_sig_SR = np.load(os.path.join(npy_dir, 'sig_in_SR-data.npy'))\n",
    "        data_sig_SB = np.load(os.path.join(npy_dir, 'sig_in_SB-data.npy'))\n",
    "        data_bkg_SR = np.load(os.path.join(npy_dir, 'bkg_in_SR-data.npy'))\n",
    "        data_bkg_SB = np.load(os.path.join(npy_dir, 'bkg_in_SB-data.npy'))\n",
    "\n",
    "        new_data_SR = np.concatenate([\n",
    "            data_sig_SR[idx_sig_SR],\n",
    "            data_bkg_SR[idx_bkg_SR],\n",
    "        ], axis=0)\n",
    "\n",
    "        if data_SR is None:\n",
    "            data_SR = new_data_SR\n",
    "        else:\n",
    "            data_SR = np.concatenate([data_SR, new_data_SR], axis=0)\n",
    "\n",
    "        new_label_SR = np.zeros(n_sig_SR + n_bkg_SR)\n",
    "        new_label_SR[:n_sig_SR] = 1\n",
    "\n",
    "        if label_SR is None:\n",
    "            label_SR = new_label_SR\n",
    "        else:\n",
    "            label_SR = np.concatenate([label_SR, new_label_SR])\n",
    "\n",
    "        new_data_SB = np.concatenate([\n",
    "            data_sig_SB[idx_sig_SB],\n",
    "            data_bkg_SB[idx_bkg_SB],\n",
    "        ], axis=0)\n",
    "\n",
    "        if data_SB is None:\n",
    "            data_SB = new_data_SB\n",
    "        else:\n",
    "            data_SB = np.concatenate([data_SB, new_data_SB], axis=0)\n",
    "\n",
    "        new_label_SB = np.zeros(n_sig_SB + n_bkg_SB)\n",
    "        new_label_SB[:n_sig_SB] = 1\n",
    "\n",
    "        if label_SB is None:\n",
    "            label_SB = new_label_SB\n",
    "        else:\n",
    "            label_SB = np.concatenate([label_SB, new_label_SB])\n",
    "\n",
    "    return data_SR, label_SR, data_SB, label_SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(path):\n",
    "    root, _ = os.path.splitext(path)\n",
    "    X = np.load(f'{root}-data.npy')\n",
    "    Y = np.load(f'{root}-label.npy')\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sculpting_sensitivity(SR_eff, SB_eff, B):\n",
    "    # SR_eff: background efficiency in signal region\n",
    "    # SB_eff: background efficiency in sideband region\n",
    "    # B: number of background events in signal region\n",
    "    nS = B * (SR_eff - SB_eff)\n",
    "    nB = B * SB_eff\n",
    "    return nS / nB**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpr_thresholds(y_true, y_scores):\n",
    "    # transform the input to numpy array\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_scores = np.asarray(y_scores).reshape(-1)\n",
    "\n",
    "    # obtain the thresholds\n",
    "    thresholds = np.sort(np.unique(y_scores))\n",
    "    \n",
    "    # get negatives index\n",
    "    negatives = (y_true == 0)\n",
    "    negatives_count = np.sum(negatives)\n",
    "    \n",
    "    # scores_matrix shape: (n_thresholds, n_samples)\n",
    "    scores_matrix = y_scores >= thresholds[:, None]\n",
    "    fp_matrix = np.sum(scores_matrix & negatives, axis=1)\n",
    "    \n",
    "    fpr_list = fp_matrix / negatives_count\n",
    "    \n",
    "    return fpr_list, thresholds\n",
    "\n",
    "\n",
    "def get_threshold_from_fpr(fpr, th, passing_rate=0.01):\n",
    "    # th 由小到大，fpr 由大到小\n",
    "    passing_rate_idx = (fpr > passing_rate).sum()\n",
    "    return th[passing_rate_idx]\n",
    "\n",
    "def get_SRfpr_from_SBfpr(X_SRSB, y_SRSB, model_name, bkg_effs=[0.1]):\n",
    "    # get the fpr in signal region from the fpr in sideband region\n",
    "    # fpr: false positive rate, background efficiency\n",
    "\n",
    "    save_model_name = f'./CNN_models/last_model_CWoLa_hunting_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "\n",
    "    X_SR, X_SB = X_SRSB\n",
    "    y_SR, y_SB = y_SRSB\n",
    "\n",
    "    y_prob_SB = loaded_model.predict(X_SB)\n",
    "    fpr_SB, th_SB = get_fpr_thresholds(y_SB == 1, y_prob_SB)\n",
    "\n",
    "    y_prob_SR = loaded_model.predict(X_SR)\n",
    "    fpr_SR, th_SR = get_fpr_thresholds(y_SR == 1, y_prob_SR)\n",
    "\n",
    "\n",
    "    bkg_effs_SR = []\n",
    "    for bkg_eff in bkg_effs:\n",
    "        th_SB_bkg_eff = get_threshold_from_fpr(fpr_SB, th_SB, bkg_eff)\n",
    "        n_th = (th_SR < th_SB_bkg_eff).sum()\n",
    "        bkg_effs_SR.append(fpr_SR[n_th])\n",
    "\n",
    "    return bkg_effs_SR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background cross section, SR: 136.13 fb, SB: 145.57 fb\n",
      "Background sample size: SR: 18922.4, SB: 20234.0\n",
      "Signal sample size: SR: 0.0, SB: 0.0\n",
      "Preparing dataset from ['../Sample/HVmodel/data/origin/25x25']\n"
     ]
    }
   ],
   "source": [
    "config_file = 'config_files/origin_config_01.json'\n",
    "# config_file = 'config_files/jet_aug_3_config_01.json'\n",
    "# config_file = 'config_files/pt_jet_aug_3_config_01.json'\n",
    "\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "train_npy_paths = config['train_npy_paths']\n",
    "val_npy_paths = config['val_npy_paths']\n",
    "seed = config['seed']\n",
    "sensitivity = config['sensitivity']\n",
    "\n",
    "true_label_path = config['true_label_path']\n",
    "model_name = config['model_name']\n",
    "sample_type = config['sample_type']\n",
    "\n",
    "# Training and validation splitting ratio\n",
    "r_train, r_val = 0.8, 0.2\n",
    "\n",
    "n_SR_S, n_SR_B, n_SB_S, n_SB_B = compute_nevent_in_SR_SB(sensitivity=sensitivity)\n",
    "\n",
    "train_nevents = int(n_SR_S * r_train), int(n_SB_S * r_train), int(n_SR_B * r_train), int(n_SB_B * r_train)\n",
    "X_train_SR, y_train_SR, X_train_SB, y_train_SB  = get_SR_SB_sample_from(train_npy_paths, train_nevents, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 10:51:00.555258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 10:51:01.183991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46699 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n",
      "2024-05-23 10:51:04.269608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/506 [>.............................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 10:51:05.906906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 6s 5ms/step\n",
      "474/474 [==============================] - 2s 5ms/step\n",
      " 0.100  0.1238  10.4\n",
      " 0.010  0.0202  14.1\n",
      " 0.001  0.0022  5.4\n"
     ]
    }
   ],
   "source": [
    "SB_effs = [0.1, 0.01, 0.001]\n",
    "SR_effs = get_SRfpr_from_SBfpr((X_train_SR, X_train_SB), (y_train_SR, y_train_SB), model_name, SB_effs)\n",
    "\n",
    "for SB_eff, SR_eff in zip(SB_effs, SR_effs):\n",
    "    print(f'{SB_eff: .3f} {SR_eff: .4f} {sculpting_sensitivity(SR_eff, SB_eff, n_SR_B): .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_SR, y_test_SR = load_samples(true_label_path)\n",
    "X_test_SB, y_test_SB = load_samples('../Sample/HVmodel/data/split_val/25x25/mix_sample_test-SB_25x25.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 5ms/step\n",
      "625/625 [==============================] - 3s 4ms/step\n",
      " 0.100  0.1071  3.1\n",
      " 0.010  0.0172  9.9\n",
      " 0.001  0.0006 -1.7\n"
     ]
    }
   ],
   "source": [
    "SR_effs = get_SRfpr_from_SBfpr((X_test_SR, X_test_SB), (y_test_SR, y_test_SB), model_name, SB_effs)\n",
    "\n",
    "for SB_eff, SR_eff in zip(SB_effs, SR_effs):\n",
    "    print(f'{SB_eff: .3f} {SR_eff: .4f} {sculpting_sensitivity(SR_eff, SB_eff, n_SR_B): .1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +3 Jet rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background cross section, SR: 136.13 fb, SB: 145.57 fb\n",
      "Background sample size: SR: 18922.4, SB: 20234.0\n",
      "Signal sample size: SR: 0.0, SB: 0.0\n",
      "Preparing dataset from ['../Sample/HVmodel/data/origin/25x25', '../Sample/HVmodel/data/jet_rotation/25x25/01', '../Sample/HVmodel/data/jet_rotation/25x25/02', '../Sample/HVmodel/data/jet_rotation/25x25/03']\n"
     ]
    }
   ],
   "source": [
    "# config_file = 'config_files/origin_config_01.json'\n",
    "config_file = 'config_files/jet_aug_3_config_01.json'\n",
    "# config_file = 'config_files/pt_jet_aug_3_config_01.json'\n",
    "\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "train_npy_paths = config['train_npy_paths']\n",
    "val_npy_paths = config['val_npy_paths']\n",
    "seed = config['seed']\n",
    "sensitivity = config['sensitivity']\n",
    "\n",
    "true_label_path = config['true_label_path']\n",
    "model_name = config['model_name']\n",
    "sample_type = config['sample_type']\n",
    "\n",
    "# Training and validation splitting ratio\n",
    "r_train, r_val = 0.8, 0.2\n",
    "\n",
    "n_SR_S, n_SR_B, n_SB_S, n_SB_B = compute_nevent_in_SR_SB(sensitivity=sensitivity)\n",
    "\n",
    "train_nevents = int(n_SR_S * r_train), int(n_SB_S * r_train), int(n_SR_B * r_train), int(n_SB_B * r_train)\n",
    "X_train_SR, y_train_SR, X_train_SB, y_train_SB  = get_SR_SB_sample_from(train_npy_paths, train_nevents, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/2024 [==============================] - 10s 5ms/step\n",
      "1893/1893 [==============================] - 9s 5ms/step\n",
      " 0.100  0.1421  18.3\n",
      " 0.010  0.0268  23.1\n",
      " 0.001  0.0011  0.5\n"
     ]
    }
   ],
   "source": [
    "SB_effs = [0.1, 0.01, 0.001]\n",
    "SR_effs = get_SRfpr_from_SBfpr((X_train_SR, X_train_SB), (y_train_SR, y_train_SB), model_name, SB_effs)\n",
    "\n",
    "for SB_eff, SR_eff in zip(SB_effs, SR_effs):\n",
    "    print(f'{SB_eff: .3f} {SR_eff: .4f} {sculpting_sensitivity(SR_eff, SB_eff, n_SR_B): .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_SR, y_test_SR = load_samples(true_label_path)\n",
    "X_test_SB, y_test_SB = load_samples('../Sample/HVmodel/data/split_val/25x25/mix_sample_test-SB_25x25.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 5ms/step\n",
      "625/625 [==============================] - 3s 5ms/step\n",
      " 0.100  0.1123  5.4\n",
      " 0.010  0.0216  16.0\n",
      " 0.001  0.0003 -3.0\n"
     ]
    }
   ],
   "source": [
    "SR_effs = get_SRfpr_from_SBfpr((X_test_SR, X_test_SB), (y_test_SR, y_test_SB), model_name, SB_effs)\n",
    "\n",
    "for SB_eff, SR_eff in zip(SB_effs, SR_effs):\n",
    "    print(f'{SB_eff: .3f} {SR_eff: .4f} {sculpting_sensitivity(SR_eff, SB_eff, n_SR_B): .1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +3 $p_\\text{T}$ smearing + Jet rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background cross section, SR: 136.13 fb, SB: 145.57 fb\n",
      "Background sample size: SR: 18922.4, SB: 20234.0\n",
      "Signal sample size: SR: 0.0, SB: 0.0\n",
      "Preparing dataset from ['../Sample/HVmodel/data/origin/25x25', '../Sample/HVmodel/data/pt_smearing_jet_rotation/25x25/01', '../Sample/HVmodel/data/pt_smearing_jet_rotation/25x25/02', '../Sample/HVmodel/data/pt_smearing_jet_rotation/25x25/03']\n"
     ]
    }
   ],
   "source": [
    "# config_file = 'config_files/origin_config_01.json'\n",
    "# config_file = 'config_files/jet_aug_3_config_01.json'\n",
    "config_file = 'config_files/pt_jet_aug_3_config_01.json'\n",
    "\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "train_npy_paths = config['train_npy_paths']\n",
    "val_npy_paths = config['val_npy_paths']\n",
    "seed = config['seed']\n",
    "sensitivity = config['sensitivity']\n",
    "\n",
    "true_label_path = config['true_label_path']\n",
    "model_name = config['model_name']\n",
    "sample_type = config['sample_type']\n",
    "\n",
    "# Training and validation splitting ratio\n",
    "r_train, r_val = 0.8, 0.2\n",
    "\n",
    "n_SR_S, n_SR_B, n_SB_S, n_SB_B = compute_nevent_in_SR_SB(sensitivity=sensitivity)\n",
    "\n",
    "train_nevents = int(n_SR_S * r_train), int(n_SB_S * r_train), int(n_SR_B * r_train), int(n_SB_B * r_train)\n",
    "X_train_SR, y_train_SR, X_train_SB, y_train_SB  = get_SR_SB_sample_from(train_npy_paths, train_nevents, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/2024 [==============================] - 10s 5ms/step\n",
      "1893/1893 [==============================] - 9s 5ms/step\n",
      " 0.100  0.1318  13.9\n",
      " 0.010  0.0212  15.4\n",
      " 0.001  0.0020  4.4\n"
     ]
    }
   ],
   "source": [
    "SB_effs = [0.1, 0.01, 0.001]\n",
    "SR_effs = get_SRfpr_from_SBfpr((X_train_SR, X_train_SB), (y_train_SR, y_train_SB), model_name, SB_effs)\n",
    "\n",
    "for SB_eff, SR_eff in zip(SB_effs, SR_effs):\n",
    "    print(f'{SB_eff: .3f} {SR_eff: .4f} {sculpting_sensitivity(SR_eff, SB_eff, n_SR_B): .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_SR, y_test_SR = load_samples(true_label_path)\n",
    "X_test_SB, y_test_SB = load_samples('../Sample/HVmodel/data/split_val/25x25/mix_sample_test-SB_25x25.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 5ms/step\n",
      "625/625 [==============================] - 3s 5ms/step\n",
      " 0.100  0.1150  6.5\n",
      " 0.010  0.0163  8.7\n",
      " 0.001  0.0011  0.4\n"
     ]
    }
   ],
   "source": [
    "SR_effs = get_SRfpr_from_SBfpr((X_test_SR, X_test_SB), (y_test_SR, y_test_SB), model_name, SB_effs)\n",
    "\n",
    "for SB_eff, SR_eff in zip(SB_effs, SR_effs):\n",
    "    print(f'{SB_eff: .3f} {SR_eff: .4f} {sculpting_sensitivity(SR_eff, SB_eff, n_SR_B): .1f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
