{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample preparation\n",
    "* Generate .root file\n",
    "* Select the events and save jet constitutent information to .h5 file\n",
    "    - Sample/from_root_to_h5.py\n",
    "* Generate mixed sample\n",
    "    - Training dataset\n",
    "    - Validation dataset\n",
    "    - True label testing dataset\n",
    "* Data augmentation\n",
    "    - optional\n",
    "    - apply on training dataset\n",
    "    - Sample/physical_augmentation_h5.ipynb\n",
    "* From .h5 file generate jet image and save in .npy file\n",
    "    - Sample/from_h5_to_npy.py\n",
    "2. CNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From root to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_root_to_h5.py ../../Software/pythia8307/HVmodel/test_100k-1.root ./HVmodel/data/split_val/signal.h5 1 &\n",
      "python from_root_to_h5.py ../../Software/pythia8307/HVmodel/test_100k-2.root ./HVmodel/data/split_val/signal-val.h5 1 &\n",
      "python from_root_to_h5.py ../../Software/pythia8307/HVmodel/test_100k-3.root ./HVmodel/data/split_val/signal-test.h5 1 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_03/tag_1_delphes_events.root ./HVmodel/data/split_val/background_03.h5 0 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_04/tag_1_delphes_events.root ./HVmodel/data/split_val/background_04.h5 0 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_05/tag_1_delphes_events.root ./HVmodel/data/split_val/background-val.h5 0 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_06/tag_1_delphes_events.root ./HVmodel/data/split_val/background-test.h5 0 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_07/tag_1_delphes_events.root ./HVmodel/data/split_val/background_07.h5 0 &\n"
     ]
    }
   ],
   "source": [
    "root_path = '../../Software/pythia8307/HVmodel/test_100k-1.root'\n",
    "output_path = './HVmodel/data/split_val/signal.h5'\n",
    "sample_type = 1\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = '../../Software/pythia8307/HVmodel/test_100k-2.root'\n",
    "output_path = './HVmodel/data/split_val/signal-val.h5'\n",
    "sample_type = 1\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = '../../Software/pythia8307/HVmodel/test_100k-3.root'\n",
    "output_path = './HVmodel/data/split_val/signal-test.h5'\n",
    "sample_type = 1\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_03/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/split_val/background_03.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_04/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/split_val/background_04.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_05/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/split_val/background-val.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_06/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/split_val/background-test.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_07/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/split_val/background_07.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate mixed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.20/08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ROOT\n",
    "\n",
    "ROOT.gROOT.ProcessLine('.include /usr/local/Delphes-3.4.2/')\n",
    "ROOT.gROOT.ProcessLine('.include /usr/local/Delphes-3.4.2/external/')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/classes/DelphesClasses.h\"')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/external/ExRootAnalysis/ExRootTreeReader.h\"')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/external/ExRootAnalysis/ExRootConfReader.h\"')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/external/ExRootAnalysis/ExRootTask.h\"')\n",
    "ROOT.gSystem.Load(\"/usr/local/Delphes-3.4.2/install/lib/libDelphes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mjets(*arg):\n",
    "    # arg: list of jets\n",
    "    # return: invariant mass of jets\n",
    "    e_tot, px_tot, py_tot, pz_tot = 0, 0, 0, 0\n",
    "    \n",
    "    for jet in arg:\n",
    "        pt, eta, phi, m = jet[0], jet[1], jet[2], jet[3]\n",
    "        \n",
    "        px, py, pz = pt*np.cos(phi), pt*np.sin(phi), pt*np.sinh(eta)\n",
    "        e = np.sqrt(m**2 + px**2 + py**2 + pz**2)\n",
    "        \n",
    "        px_tot += px\n",
    "        py_tot += py\n",
    "        pz_tot += pz\n",
    "        e_tot += e\n",
    "    \n",
    "    return np.sqrt(e_tot**2 - px_tot**2 - py_tot**2 - pz_tot**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HV_selection(tree):\n",
    "    # Hidden Valley model selection\n",
    "    # 1. 2 jets\n",
    "    # 2. pT > 750 GeV\n",
    "    # 3. |eta| < 2.0\n",
    "\n",
    "    SRSB_region = [4400, 4700, 5500, 5800]\n",
    "\n",
    "    n_event_count = 0\n",
    "    n_jet_count = 0\n",
    "    jet_pt_count = 0\n",
    "    jet_eta_count = 0\n",
    "    mjjs = []\n",
    "\n",
    "    pt = [[],[]]\n",
    "\n",
    "    for event_id, event in tqdm(enumerate(tree)):\n",
    "        n_event_count += 1\n",
    "\n",
    "        if event.Jet_size < 2:\n",
    "            continue\n",
    "        n_jet_count += 1\n",
    "\n",
    "        pt[0].append(event.Jet[0].PT)\n",
    "        pt[1].append(event.Jet[1].PT)\n",
    "        if event.Jet[1].PT < 750:\n",
    "            continue\n",
    "        jet_pt_count += 1\n",
    "\n",
    "        if abs(event.Jet[0].Eta) > 2.0 or abs(event.Jet[1].Eta) > 2.0:\n",
    "            continue\n",
    "        jet_eta_count += 1\n",
    "\n",
    "        jets = [[event.Jet[i].PT, event.Jet[i].Eta, event.Jet[i].Phi, event.Jet[i].Mass] for i in range(2)]\n",
    "        mjj = Mjets(*jets)\n",
    "        mjjs.append(mjj)\n",
    "\n",
    "        if mjj < SRSB_region[0] or mjj > SRSB_region[3]:\n",
    "            continue\n",
    "\n",
    "\n",
    "    mjjs = np.array(mjjs)\n",
    "    SR_count = ((mjjs > SRSB_region[1]) & (mjjs < SRSB_region[2])).sum()\n",
    "    SB_count = (((mjjs > SRSB_region[0]) & (mjjs < SRSB_region[1])) | ((mjjs > SRSB_region[2]) & (mjjs < SRSB_region[3]))).sum()\n",
    "\n",
    "    cutflow_number = {\n",
    "        'Total': n_event_count,\n",
    "        'n jet cut': n_jet_count,\n",
    "        'jet pt cut': jet_pt_count,\n",
    "        'jet eta cut': jet_eta_count,\n",
    "        'Signal region': SR_count,\n",
    "        'Sideband region': SB_count,\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        'mjj': mjjs,\n",
    "        'pt': np.array(pt),\n",
    "        'cutflow_number': cutflow_number,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [08:58, 185.74it/s]\n"
     ]
    }
   ],
   "source": [
    "root_file = '../Software/pythia8307/HVmodel/test_100k-1.root'\n",
    "f = ROOT.TFile(root_file)\n",
    "tree_s = f.Get(\"Delphes\")\n",
    "\n",
    "results_s = HV_selection(tree_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [07:45, 215.05it/s]\n",
      "Warning in <TStreamerInfo::BuildCheck>: \n",
      "   The StreamerInfo for version 2 of class GenParticle read from the file ./Sample/ppjj/Events/run_02/tag_1_delphes_events.root\n",
      "   has a different checksum than the previously loaded StreamerInfo.\n",
      "   Reading objects of type GenParticle from the file ./Sample/ppjj/Events/run_02/tag_1_delphes_events.root \n",
      "   (and potentially other files) might not work correctly.\n",
      "   Most likely the version number of the class was not properly\n",
      "   updated [See ClassDef(GenParticle,2)].\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float T; //number\n",
      "vs\n",
      "   float CtgTheta; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float X; //number\n",
      "vs\n",
      "   float D0; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float Y; //number\n",
      "vs\n",
      "   float DZ; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float Z; //number\n",
      "vs\n",
      "   float T; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 2 of class 'GenParticle' is missing from \n",
      "the on-file layout version 2:\n",
      "   float X; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 2 of class 'GenParticle' is missing from \n",
      "the on-file layout version 2:\n",
      "   float Y; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 2 of class 'GenParticle' is missing from \n",
      "the on-file layout version 2:\n",
      "   float Z; //number\n",
      "Error in <TObjArray::At>: index 33 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 34 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 35 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 36 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 37 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 38 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 39 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 40 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 41 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 42 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 43 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 44 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 45 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 46 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Error in <TObjArray::At>: index 47 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TStreamerInfo::BuildCheck>: \n",
      "   The StreamerInfo for version 3 of class Track read from the file ./Sample/ppjj/Events/run_02/tag_1_delphes_events.root\n",
      "   has a different checksum than the previously loaded StreamerInfo.\n",
      "   Reading objects of type Track from the file ./Sample/ppjj/Events/run_02/tag_1_delphes_events.root \n",
      "   (and potentially other files) might not work correctly.\n",
      "   Most likely the version number of the class was not properly\n",
      "   updated [See ClassDef(Track,3)].\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float C; //\n",
      "vs\n",
      "   float EtaOuter; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Mass; //\n",
      "vs\n",
      "   float PhiOuter; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float EtaOuter; //\n",
      "vs\n",
      "   float T; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float PhiOuter; //\n",
      "vs\n",
      "   float X; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float T; //\n",
      "vs\n",
      "   float Y; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float X; //\n",
      "vs\n",
      "   float Z; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Y; //\n",
      "vs\n",
      "   float TOuter; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Z; //\n",
      "vs\n",
      "   float XOuter; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float TOuter; //\n",
      "vs\n",
      "   float YOuter; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float XOuter; //\n",
      "vs\n",
      "   float ZOuter; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float YOuter; //\n",
      "vs\n",
      "   float Xd; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float ZOuter; //\n",
      "vs\n",
      "   float Yd; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Xd; //\n",
      "vs\n",
      "   float Zd; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Yd; //\n",
      "vs\n",
      "   float L; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Zd; //\n",
      "vs\n",
      "   float D0; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float L; //\n",
      "vs\n",
      "   float DZ; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float D0; //\n",
      "vs\n",
      "   float ErrorP; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float DZ; //\n",
      "vs\n",
      "   float ErrorPT; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float Nclusters; //\n",
      "vs\n",
      "   float ErrorPhi; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float dNdx; //\n",
      "vs\n",
      "   float ErrorCtgTheta; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float ErrorP; //\n",
      "vs\n",
      "   float ErrorT; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float ErrorPT; //\n",
      "vs\n",
      "   float ErrorD0; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float ErrorPhi; //\n",
      "vs\n",
      "   float ErrorDZ; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float ErrorCtgTheta; //\n",
      "vs\n",
      "   TRef Particle; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 3 of class 'Track' differs from \n",
      "the in-memory layout version 3:\n",
      "   float ErrorT; //\n",
      "vs\n",
      "   int VertexIndex; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorD0; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorDZ; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorC; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorD0Phi; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorD0C; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorD0DZ; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorD0CtgTheta; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorPhiC; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorPhiDZ; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorPhiCtgTheta; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorCDZ; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorCCtgTheta; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   float ErrorDZCtgTheta; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   TRef Particle; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 3 of class 'Track' is missing from \n",
      "the on-file layout version 3:\n",
      "   int VertexIndex; //\n",
      "Error in <TObjArray::At>: index 42 out of bounds (size: 42, this: 0x5579dd690180)\n",
      "Error in <TObjArray::At>: index 43 out of bounds (size: 42, this: 0x5579dd690180)\n",
      "Warning in <TStreamerInfo::BuildCheck>: \n",
      "   The StreamerInfo for version 4 of class Jet read from the file ./Sample/ppjj/Events/run_02/tag_1_delphes_events.root\n",
      "   has a different checksum than the previously loaded StreamerInfo.\n",
      "   Reading objects of type Jet from the file ./Sample/ppjj/Events/run_02/tag_1_delphes_events.root \n",
      "   (and potentially other files) might not work correctly.\n",
      "   Most likely the version number of the class was not properly\n",
      "   updated [See ClassDef(Jet,4)].\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float NeutralEnergyFraction; //\n",
      "vs\n",
      "   float Beta; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float ChargedEnergyFraction; //\n",
      "vs\n",
      "   float BetaStar; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float Beta; //\n",
      "vs\n",
      "   float MeanSqDeltaR; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float BetaStar; //\n",
      "vs\n",
      "   float PTD; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float MeanSqDeltaR; //\n",
      "vs\n",
      "   float FracPt; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float PTD; //\n",
      "vs\n",
      "   float Tau; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float FracPt; //\n",
      "vs\n",
      "   TLorentzVector SoftDroppedJet; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   float Tau; //\n",
      "vs\n",
      "   TLorentzVector SoftDroppedSubJet1; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TLorentzVector SoftDroppedJet; //\n",
      "vs\n",
      "   TLorentzVector SoftDroppedSubJet2; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TLorentzVector SoftDroppedSubJet1; //\n",
      "vs\n",
      "   TLorentzVector TrimmedP4; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TLorentzVector SoftDroppedSubJet2; //\n",
      "vs\n",
      "   TLorentzVector PrunedP4; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TLorentzVector TrimmedP4; //\n",
      "vs\n",
      "   TLorentzVector SoftDroppedP4; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TLorentzVector PrunedP4; //\n",
      "vs\n",
      "   int NSubJetsTrimmed; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TLorentzVector SoftDroppedP4; //\n",
      "vs\n",
      "   int NSubJetsPruned; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   int NSubJetsTrimmed; //\n",
      "vs\n",
      "   int NSubJetsSoftDropped; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   int NSubJetsPruned; //\n",
      "vs\n",
      "   double ExclYmerge23; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   int NSubJetsSoftDropped; //\n",
      "vs\n",
      "   double ExclYmerge34; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   double ExclYmerge23; //\n",
      "vs\n",
      "   double ExclYmerge45; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   double ExclYmerge34; //\n",
      "vs\n",
      "   double ExclYmerge56; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   double ExclYmerge45; //\n",
      "vs\n",
      "   TRefArray Constituents; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   double ExclYmerge56; //\n",
      "vs\n",
      "   TRefArray Particles; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 4 of class 'Jet' differs from \n",
      "the in-memory layout version 4:\n",
      "   TRefArray Constituents; //\n",
      "vs\n",
      "   TLorentzVector Area; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 4 of class 'Jet' is missing from \n",
      "the on-file layout version 4:\n",
      "   TRefArray Particles; //\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 4 of class 'Jet' is missing from \n",
      "the on-file layout version 4:\n",
      "   TLorentzVector Area; //\n",
      "Error in <TObjArray::At>: index 35 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorC\n",
      "Error in <TObjArray::At>: index 36 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorD0Phi\n",
      "Error in <TObjArray::At>: index 37 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorD0C\n",
      "Error in <TObjArray::At>: index 38 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorD0DZ\n",
      "Error in <TObjArray::At>: index 39 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorD0CtgTheta\n",
      "Error in <TObjArray::At>: index 40 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorPhiC\n",
      "Error in <TObjArray::At>: index 41 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorPhiDZ\n",
      "Error in <TObjArray::At>: index 42 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorPhiCtgTheta\n",
      "Error in <TObjArray::At>: index 43 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorCDZ\n",
      "Error in <TObjArray::At>: index 44 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorCCtgTheta\n",
      "Error in <TObjArray::At>: index 45 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: Track subbranch: Track.ErrorDZCtgTheta\n",
      "Error in <TObjArray::At>: index 35 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorC\n",
      "Error in <TObjArray::At>: index 36 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorD0Phi\n",
      "Error in <TObjArray::At>: index 37 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorD0C\n",
      "Error in <TObjArray::At>: index 38 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorD0DZ\n",
      "Error in <TObjArray::At>: index 39 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorD0CtgTheta\n",
      "Error in <TObjArray::At>: index 40 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorPhiC\n",
      "Error in <TObjArray::At>: index 41 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorPhiDZ\n",
      "Error in <TObjArray::At>: index 42 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorPhiCtgTheta\n",
      "Error in <TObjArray::At>: index 43 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorCDZ\n",
      "Error in <TObjArray::At>: index 44 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorCCtgTheta\n",
      "Error in <TObjArray::At>: index 45 out of bounds (size: 33, this: 0x5579dd5e5fa0)\n",
      "Warning in <TBranchElement::InitializeOffsets>: No streamer element for branch: EFlowTrack subbranch: EFlowTrack.ErrorDZCtgTheta\n"
     ]
    }
   ],
   "source": [
    "root_file = './Sample/ppjj/Events/run_02/tag_1_delphes_events.root'\n",
    "f = ROOT.TFile(root_file)\n",
    "tree_b = f.Get(\"Delphes\")\n",
    "\n",
    "results_b = HV_selection(tree_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified SB region to 4400-4700, 5500-5800\n",
    "np.save('./Sample/HVmodel/data/selection_results_SB_4400_5800_s.npy', results_s)\n",
    "np.save('./Sample/HVmodel/data/selection_results_SB_4400_5800_b.npy', results_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path):\n",
    "    # path: run path\n",
    "    name = os.path.split(path)[1]\n",
    "\n",
    "    with open(os.path.join(path, f'{name}_tag_1_banner.txt')) as f:\n",
    "        for line in f.readlines():\n",
    "                \n",
    "            #  Integrated weight (pb)  :       0.020257\n",
    "            match = re.match('#  Integrated weight \\(pb\\)  : +(\\d+\\.\\d+)', line)\n",
    "            if match:\n",
    "                # unit: fb\n",
    "                cross_section = float(match.group(1)) * 1000\n",
    "            # #  Number of Events        :       100000\n",
    "            match = re.match('#  Number of Events        :       (\\d+)', line)\n",
    "            if match:\n",
    "                # unit: fb\n",
    "                nevent = int(match.group(1))\n",
    "    \n",
    "    return cross_section, nevent\n",
    "\n",
    "def get_dataset_keys(f):\n",
    "    keys = []\n",
    "    f.visit(lambda key : keys.append(key) if isinstance(f[key], h5py.Dataset) else None)\n",
    "    return keys\n",
    "\n",
    "def create_dataset(f, nevent, MAX_JETS):\n",
    "\n",
    "    f.create_dataset('J1/MASK', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='|b1')\n",
    "    f.create_dataset('J1/pt', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J1/eta', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J1/phi', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "\n",
    "    f.create_dataset('J2/MASK', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='|b1')\n",
    "    f.create_dataset('J2/pt', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J2/eta', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J2/phi', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "\n",
    "    f.create_dataset('EVENT/Mjj', (nevent,), maxshape=(None,), dtype='<f4')\n",
    "    f.create_dataset('EVENT/signal', (nevent,), maxshape=(None,), dtype='<i8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_s = np.load('./Sample/HVmodel/data/selection_results_SB_4400_5800_s.npy', allow_pickle=True).item()\n",
    "results_b = np.load('./Sample/HVmodel/data/selection_results_SB_4400_5800_b.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6837.392481 1000000\n",
      "Background cross section, SR: 136.13 fb, SB: 145.57 fb\n",
      "Background sample size: SR: 75689.7, SB: 80935.9\n",
      "275.11754082386454 67.00583183055264\n"
     ]
    }
   ],
   "source": [
    "# Total cross section and number of events\n",
    "xection, tot_event = get_info('./Sample/ppjj/Events/run_03')\n",
    "print(xection, tot_event)\n",
    "\n",
    "# cross section in signal region and sideband region\n",
    "cross_section_SR = results_b['cutflow_number']['Signal region'] / results_b['cutflow_number']['Total'] * xection\n",
    "cross_section_SB = results_b['cutflow_number']['Sideband region'] / results_b['cutflow_number']['Total'] * xection\n",
    "print(f'Background cross section, SR: {cross_section_SR:.2f} fb, SB: {cross_section_SB:.2f} fb')\n",
    "\n",
    "# number of background events in signal region and sideband region\n",
    "L = 139 * 4\n",
    "n_SR_B = cross_section_SR * L\n",
    "n_SB_B = cross_section_SB * L\n",
    "\n",
    "print(f'Background sample size: SR: {n_SR_B:.1f}, SB: {n_SB_B:.1f}')\n",
    "\n",
    "sensitivity = 1\n",
    "n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "print(n_SR_S, n_SB_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation splitting ratio\n",
    "r_train = 0.8\n",
    "r_val = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path, SRSB_region=[4400, 4700, 5500, 5800]):\n",
    "    # n_sig_1: number of signal events in mixing sample 1 (Signal region)\n",
    "    # n_sig_2: number of signal events in mixing sample 2 (Sideband region)\n",
    "\n",
    "    nevent = n_sig_1 + n_sig_2 + n_bkg_1 + n_bkg_2\n",
    "    with h5py.File(output_path, 'w') as f_out:\n",
    "        MAX_JETS = 300 \n",
    "        create_dataset(f_out, nevent, MAX_JETS)\n",
    "\n",
    "        keys = get_dataset_keys(f_out)\n",
    "        with h5py.File(sig_path, 'r') as f_sig, h5py.File(bkg_path, 'r') as f_bkg:  \n",
    "            mjj_s = f_sig['EVENT/Mjj'][:]\n",
    "            mjj_b = f_bkg['EVENT/Mjj'][:]\n",
    "            SR_range_s = (mjj_s > SRSB_region[1]) & (mjj_s < SRSB_region[2])\n",
    "            SB_range_s = ((mjj_s > SRSB_region[0]) & (mjj_s < SRSB_region[1])) | ((mjj_s > SRSB_region[2]) & (mjj_s < SRSB_region[3]))\n",
    "            SR_range_b = (mjj_b > SRSB_region[1]) & (mjj_b < SRSB_region[2])\n",
    "            SB_range_b = ((mjj_b > SRSB_region[0]) & (mjj_b < SRSB_region[1])) | ((mjj_b > SRSB_region[2]) & (mjj_b < SRSB_region[3]))\n",
    "\n",
    "            for key in keys:\n",
    "                f_out[key][:n_sig_1] = f_sig[key][:][SR_range_s][:n_sig_1]\n",
    "                f_out[key][n_sig_1:n_sig_1+n_bkg_1] = f_bkg[key][:][SR_range_b][:n_bkg_1]\n",
    "                f_out[key][n_sig_1+n_bkg_1:n_sig_1+n_bkg_1+n_sig_2] = f_sig[key][:][SB_range_s][:n_sig_2]\n",
    "                f_out[key][n_sig_1+n_bkg_1+n_sig_2:] = f_bkg[key][:][SB_range_b][:n_bkg_2]\n",
    "\n",
    "        f_out['EVENT/signal'][:n_sig_1+n_bkg_1] = 1\n",
    "        f_out['EVENT/signal'][n_sig_1+n_bkg_1:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original mix sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data process flow\n",
    "# background.h5 merged from background_03.h5 background_04.h5 files\n",
    "sig_path = './Sample/HVmodel/data/split_val/signal.h5'\n",
    "bkg_path = './Sample/HVmodel/data/split_val/background.h5'\n",
    "\n",
    "for i in range(11):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S * r_train), int(n_SB_S * r_train), int(n_SR_B * r_train), int(n_SB_B * r_train)\n",
    "    output_path = f'./Sample/HVmodel/data/split_val/mix_sample_{i:.1f}.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luminosity $\\times 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path = './Sample/HVmodel/data/split_val/signal.h5'\n",
    "bkg_path = './Sample/HVmodel/data/split_val/background.h5'\n",
    "\n",
    "for i in range(11):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S * r_train), int(n_SB_S * r_train), int(n_SR_B * r_train), int(n_SB_B * r_train)\n",
    "    output_path = f'./Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_x2.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39548 42189\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't broadcast (39548,) -> (60551,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_SR_S \u001b[38;5;241m*\u001b[39m r_train), \u001b[38;5;28mint\u001b[39m(n_SB_S \u001b[38;5;241m*\u001b[39m r_train), \u001b[38;5;28mint\u001b[39m(n_SR_B \u001b[38;5;241m*\u001b[39m r_train), \u001b[38;5;28mint\u001b[39m(n_SB_B \u001b[38;5;241m*\u001b[39m r_train)\n\u001b[1;32m     10\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Sample/HVmodel/data/split_val/mix_sample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_x4.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcreate_mix_sample_from_numbers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbkg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sig_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sig_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bkg_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bkg_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m, in \u001b[0;36mcreate_mix_sample_from_numbers\u001b[0;34m(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path, SRSB_region)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[1;32m     20\u001b[0m     f_out[key][:n_sig_1] \u001b[38;5;241m=\u001b[39m f_sig[key][:][SR_range_s][:n_sig_1]\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mf_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_sig_1\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_sig_1\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn_bkg_1\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m f_bkg[key][:][SR_range_b][:n_bkg_1]\n\u001b[1;32m     22\u001b[0m     f_out[key][n_sig_1\u001b[38;5;241m+\u001b[39mn_bkg_1:n_sig_1\u001b[38;5;241m+\u001b[39mn_bkg_1\u001b[38;5;241m+\u001b[39mn_sig_2] \u001b[38;5;241m=\u001b[39m f_sig[key][:][SB_range_s][:n_sig_2]\n\u001b[1;32m     23\u001b[0m     f_out[key][n_sig_1\u001b[38;5;241m+\u001b[39mn_bkg_1\u001b[38;5;241m+\u001b[39mn_sig_2:] \u001b[38;5;241m=\u001b[39m f_bkg[key][:][SB_range_b][:n_bkg_2]\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.8/site-packages/h5py/_hl/dataset.py:949\u001b[0m, in \u001b[0;36mDataset.__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    946\u001b[0m     mshape \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# Perform the write, with broadcasting\u001b[39;00m\n\u001b[0;32m--> 949\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(\u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmshape\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fspace \u001b[38;5;129;01min\u001b[39;00m selection\u001b[38;5;241m.\u001b[39mbroadcast(mshape):\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mwrite(mspace, fspace, val, mtype, dxpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dxpl)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.8/site-packages/h5py/_hl/selections.py:264\u001b[0m, in \u001b[0;36mSimpleSelection.expand_shape\u001b[0;34m(self, source_shape)\u001b[0m\n\u001b[1;32m    262\u001b[0m             eshape\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt broadcast \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (source_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray_shape))  \u001b[38;5;66;03m# array shape\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m remaining_src_dims]):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# All dimensions from target_shape should either have been popped\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# to match the selection shape, or be 1.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt broadcast \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (source_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray_shape))  \u001b[38;5;66;03m# array shape\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't broadcast (39548,) -> (60551,)"
     ]
    }
   ],
   "source": [
    "sig_path = './Sample/HVmodel/data/split_val/signal.h5'\n",
    "bkg_path = './Sample/HVmodel/data/split_val/background.h5'\n",
    "\n",
    "for i in range(1):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S * r_train), int(n_SB_S * r_train), int(n_SR_B * r_train), int(n_SB_B * r_train)\n",
    "    output_path = f'./Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_x4.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path = './Sample/HVmodel/data/split_val/signal-val.h5'\n",
    "bkg_path = './Sample/HVmodel/data/split_val/background-val.h5'\n",
    "\n",
    "for i in range(11):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S * r_val), int(n_SB_S * r_val), int(n_SR_B * r_val), int(n_SB_B * r_val)\n",
    "    output_path = f'./Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path, SRSB_region=[4400, 4700, 5500, 5800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luminosity $\\times 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path = './Sample/HVmodel/data/split_val/signal-val.h5'\n",
    "bkg_path = './Sample/HVmodel/data/split_val/background-val.h5'\n",
    "\n",
    "for i in range(11):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S * r_val), int(n_SB_S * r_val), int(n_SR_B * r_val), int(n_SB_B * r_val)\n",
    "    output_path = f'./Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_x2_val.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path, SRSB_region=[4400, 4700, 5500, 5800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path, bkg_path  = './Sample/HVmodel/data/split_val/signal-test.h5', './Sample/HVmodel/data/split_val/background-test.h5'\n",
    "n_sig_1, n_bkg_1 = 10000, 10000\n",
    "nevent = n_sig_1 + n_bkg_1\n",
    "\n",
    "with h5py.File('./Sample/HVmodel/data/split_val/mix_sample_test.h5', 'w') as f_out:\n",
    "    \n",
    "    MAX_JETS = 300 \n",
    "    create_dataset(f_out, nevent, MAX_JETS)\n",
    "\n",
    "    keys = get_dataset_keys(f_out)\n",
    "    with h5py.File(sig_path, 'r') as f_sig, h5py.File(bkg_path, 'r') as f_bkg:  \n",
    "        mjj_s = f_sig['EVENT/Mjj'][:]\n",
    "        mjj_b = f_bkg['EVENT/Mjj'][:]\n",
    "        SR_range_s = (mjj_s > 4700) & (mjj_s < 5500)\n",
    "        SR_range_b = (mjj_b > 4700) & (mjj_b < 5500)\n",
    "\n",
    "        for key in keys:\n",
    "            f_out[key][:n_sig_1] = f_sig[key][:][SR_range_s][:n_sig_1]\n",
    "            f_out[key][n_sig_1:] = f_bkg[key][:][SR_range_b][:n_bkg_1]\n",
    "\n",
    "    f_out['EVENT/signal'][:n_sig_1] = 1\n",
    "    f_out['EVENT/signal'][n_sig_1:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path, bkg_path  = './Sample/HVmodel/data/new/signal-test-2.h5', './Sample/HVmodel/data/new/background-test-2.h5'\n",
    "n_sig_1, n_bkg_1 = 10000, 10000\n",
    "nevent = n_sig_1 + n_bkg_1\n",
    "\n",
    "with h5py.File('./Sample/HVmodel/data/new/mix_sample_testing-2.h5', 'w') as f_out:\n",
    "    \n",
    "    MAX_JETS = 300 \n",
    "    create_dataset(f_out, nevent, MAX_JETS)\n",
    "\n",
    "    keys = get_dataset_keys(f_out)\n",
    "    with h5py.File(sig_path, 'r') as f_sig, h5py.File(bkg_path, 'r') as f_bkg:  \n",
    "        mjj_s = f_sig['EVENT/Mjj'][:]\n",
    "        mjj_b = f_bkg['EVENT/Mjj'][:]\n",
    "        SR_range_s = (mjj_s > 4700) & (mjj_s < 5500)\n",
    "        SR_range_b = (mjj_b > 4700) & (mjj_b < 5500)\n",
    "\n",
    "        for key in keys:\n",
    "            f_out[key][:n_sig_1] = f_sig[key][:][SR_range_s][:n_sig_1]\n",
    "            f_out[key][n_sig_1:] = f_bkg[key][:][SR_range_b][:n_bkg_1]\n",
    "\n",
    "    f_out['EVENT/signal'][:n_sig_1] = 1\n",
    "    f_out['EVENT/signal'][n_sig_1:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Sample/physical_augmentation_h5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From h5 to npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution $75 \\times 75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original mix sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0.h5 ./HVmodel/data/split_val/mix_sample_0.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0.h5 ./HVmodel/data/split_val/mix_sample_1.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0.h5 ./HVmodel/data/split_val/mix_sample_2.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0.h5 ./HVmodel/data/split_val/mix_sample_3.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0.h5 ./HVmodel/data/split_val/mix_sample_4.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0.h5 ./HVmodel/data/split_val/mix_sample_5.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0.h5 ./HVmodel/data/split_val/mix_sample_6.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0.h5 ./HVmodel/data/split_val/mix_sample_7.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0.h5 ./HVmodel/data/split_val/mix_sample_8.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0.h5 ./HVmodel/data/split_val/mix_sample_9.0_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0.h5 ./HVmodel/data/split_val/mix_sample_10.0_75x75.npy 75 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_75x75.npy'\n",
    "    resolution = 75\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_val.h5 ./HVmodel/data/split_val/mix_sample_0.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_val.h5 ./HVmodel/data/split_val/mix_sample_1.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_val.h5 ./HVmodel/data/split_val/mix_sample_2.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_val.h5 ./HVmodel/data/split_val/mix_sample_3.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_val.h5 ./HVmodel/data/split_val/mix_sample_4.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_val.h5 ./HVmodel/data/split_val/mix_sample_5.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_val.h5 ./HVmodel/data/split_val/mix_sample_6.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_val.h5 ./HVmodel/data/split_val/mix_sample_7.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_val.h5 ./HVmodel/data/split_val/mix_sample_8.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_val.h5 ./HVmodel/data/split_val/mix_sample_9.0_val_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_val.h5 ./HVmodel/data/split_val/mix_sample_10.0_val_75x75.npy 75 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_val.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_val_75x75.npy'\n",
    "    resolution = 75\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_test.h5 ./HVmodel/data/split_val/mix_sample_test_75x75.npy 75 &\n"
     ]
    }
   ],
   "source": [
    "h5_path = f'./HVmodel/data/split_val/mix_sample_test.h5'\n",
    "output_path = f'./HVmodel/data/split_val/mix_sample_test_75x75.npy'\n",
    "resolution = 75\n",
    "\n",
    "cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy: +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_0.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_1.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_2.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_3.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_4.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_5.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_6.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_7.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_8.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_9.0_copy_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_10.0_copy_1_75x75.npy 75 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_copy_1.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_copy_1_75x75.npy'\n",
    "    resolution = 75\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation: +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_0.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_1.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_2.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_3.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_4.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_5.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_6.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_7.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_8.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_9.0_aug_1_75x75.npy 75 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_10.0_aug_1_75x75.npy 75 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_aug_1.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_aug_1_75x75.npy'\n",
    "    resolution = 75\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luminosity $\\times 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_0.0_x2.h5 ./HVmodel/data/new/mix_sample_0.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_1.0_x2.h5 ./HVmodel/data/new/mix_sample_1.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_2.0_x2.h5 ./HVmodel/data/new/mix_sample_2.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_3.0_x2.h5 ./HVmodel/data/new/mix_sample_3.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_4.0_x2.h5 ./HVmodel/data/new/mix_sample_4.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_5.0_x2.h5 ./HVmodel/data/new/mix_sample_5.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_6.0_x2.h5 ./HVmodel/data/new/mix_sample_6.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_7.0_x2.h5 ./HVmodel/data/new/mix_sample_7.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_8.0_x2.h5 ./HVmodel/data/new/mix_sample_8.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_9.0_x2.h5 ./HVmodel/data/new/mix_sample_9.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_10.0_x2.h5 ./HVmodel/data/new/mix_sample_10.0_x2.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_x2.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_x2.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_11.0.h5 ./HVmodel/data/new/mix_sample_11.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_12.0.h5 ./HVmodel/data/new/mix_sample_12.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_13.0.h5 ./HVmodel/data/new/mix_sample_13.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_14.0.h5 ./HVmodel/data/new/mix_sample_14.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_15.0.h5 ./HVmodel/data/new/mix_sample_15.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_16.0.h5 ./HVmodel/data/new/mix_sample_16.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_17.0.h5 ./HVmodel/data/new/mix_sample_17.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_18.0.h5 ./HVmodel/data/new/mix_sample_18.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_19.0.h5 ./HVmodel/data/new/mix_sample_19.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_20.0.h5 ./HVmodel/data/new/mix_sample_20.0.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11, 21):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_0.0_copy_1.h5 ./HVmodel/data/new/mix_sample_0.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_1.0_copy_1.h5 ./HVmodel/data/new/mix_sample_1.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_2.0_copy_1.h5 ./HVmodel/data/new/mix_sample_2.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_3.0_copy_1.h5 ./HVmodel/data/new/mix_sample_3.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_4.0_copy_1.h5 ./HVmodel/data/new/mix_sample_4.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_5.0_copy_1.h5 ./HVmodel/data/new/mix_sample_5.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_6.0_copy_1.h5 ./HVmodel/data/new/mix_sample_6.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_7.0_copy_1.h5 ./HVmodel/data/new/mix_sample_7.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_8.0_copy_1.h5 ./HVmodel/data/new/mix_sample_8.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_9.0_copy_1.h5 ./HVmodel/data/new/mix_sample_9.0_copy_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_10.0_copy_1.h5 ./HVmodel/data/new/mix_sample_10.0_copy_1.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_copy_1.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_copy_1.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_0.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_0.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_1.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_1.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_2.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_2.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_3.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_3.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_4.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_4.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_5.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_5.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_6.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_6.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_7.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_7.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_8.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_8.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_9.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_9.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_10.0_aug_1.h5 ./HVmodel/data/aug_only/mix_sample_10.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_0.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_0.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_1.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_1.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_2.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_2.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_3.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_3.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_4.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_4.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_5.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_5.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_6.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_6.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_7.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_7.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_8.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_8.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_9.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_9.0_aug_2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/aug_only/mix_sample_10.0_aug_2.h5 ./HVmodel/data/aug_only/mix_sample_10.0_aug_2.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/aug_only/mix_sample_{i:.1f}_aug_1.h5'\n",
    "    output_path = f'./HVmodel/data/aug_only/mix_sample_{i:.1f}_aug_1.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)\n",
    "\n",
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/aug_only/mix_sample_{i:.1f}_aug_2.h5'\n",
    "    output_path = f'./HVmodel/data/aug_only/mix_sample_{i:.1f}_aug_2.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smearing scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_0.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_0.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_1.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_1.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_2.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_2.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_3.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_3.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_4.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_4.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_5.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_5.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_6.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_6.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_7.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_7.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_8.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_8.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_9.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_9.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_02/mix_sample_10.0_aug_1.h5 ./HVmodel/data/smearing_scale_02/mix_sample_10.0_aug_1.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/smearing_scale_02/mix_sample_{i:.1f}_aug_1.h5'\n",
    "    output_path = f'./HVmodel/data/smearing_scale_02/mix_sample_{i:.1f}_aug_1.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_0.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_0.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_1.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_1.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_2.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_2.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_3.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_3.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_4.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_4.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_5.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_5.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_6.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_6.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_7.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_7.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_8.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_8.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_9.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_9.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/smearing_scale_05/mix_sample_10.0_aug_1.h5 ./HVmodel/data/smearing_scale_05/mix_sample_10.0_aug_1.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/smearing_scale_05/mix_sample_{i:.1f}_aug_1.h5'\n",
    "    output_path = f'./HVmodel/data/smearing_scale_05/mix_sample_{i:.1f}_aug_1.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Half copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_0.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_0.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_1.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_1.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_2.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_2.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_3.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_3.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_4.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_4.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_5.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_5.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_6.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_6.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_7.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_7.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_8.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_8.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_9.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_9.0_half_copy_1.h5 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_10.0_half_copy_1.h5 ./HVmodel/data/new/mix_sample_10.0_half_copy_1.h5 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_half_copy_1.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_half_copy_1.h5'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution $25 \\times 25$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original mix sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0.h5 ./HVmodel/data/split_val/mix_sample_0.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0.h5 ./HVmodel/data/split_val/mix_sample_1.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0.h5 ./HVmodel/data/split_val/mix_sample_2.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0.h5 ./HVmodel/data/split_val/mix_sample_3.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0.h5 ./HVmodel/data/split_val/mix_sample_4.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0.h5 ./HVmodel/data/split_val/mix_sample_5.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0.h5 ./HVmodel/data/split_val/mix_sample_6.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0.h5 ./HVmodel/data/split_val/mix_sample_7.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0.h5 ./HVmodel/data/split_val/mix_sample_8.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0.h5 ./HVmodel/data/split_val/mix_sample_9.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0.h5 ./HVmodel/data/split_val/mix_sample_10.0_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_val.h5 ./HVmodel/data/split_val/mix_sample_0.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_val.h5 ./HVmodel/data/split_val/mix_sample_1.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_val.h5 ./HVmodel/data/split_val/mix_sample_2.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_val.h5 ./HVmodel/data/split_val/mix_sample_3.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_val.h5 ./HVmodel/data/split_val/mix_sample_4.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_val.h5 ./HVmodel/data/split_val/mix_sample_5.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_val.h5 ./HVmodel/data/split_val/mix_sample_6.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_val.h5 ./HVmodel/data/split_val/mix_sample_7.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_val.h5 ./HVmodel/data/split_val/mix_sample_8.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_val.h5 ./HVmodel/data/split_val/mix_sample_9.0_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_val.h5 ./HVmodel/data/split_val/mix_sample_10.0_val_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_val.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_val_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_test.h5 ./HVmodel/data/split_val/mix_sample_test_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "h5_path = f'./HVmodel/data/split_val/mix_sample_test.h5'\n",
    "output_path = f'./HVmodel/data/split_val/mix_sample_test_25x25.npy'\n",
    "resolution = 25\n",
    "\n",
    "cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy: +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_0.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_1.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_2.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_3.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_4.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_5.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_6.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_7.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_8.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_9.0_copy_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_copy_1.h5 ./HVmodel/data/split_val/mix_sample_10.0_copy_1_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_copy_1.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_copy_1_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luminosity: $\\times 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_x2.h5 ./HVmodel/data/split_val/mix_sample_0.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_x2.h5 ./HVmodel/data/split_val/mix_sample_1.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_x2.h5 ./HVmodel/data/split_val/mix_sample_2.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_x2.h5 ./HVmodel/data/split_val/mix_sample_3.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_x2.h5 ./HVmodel/data/split_val/mix_sample_4.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_x2.h5 ./HVmodel/data/split_val/mix_sample_5.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_x2.h5 ./HVmodel/data/split_val/mix_sample_6.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_x2.h5 ./HVmodel/data/split_val/mix_sample_7.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_x2.h5 ./HVmodel/data/split_val/mix_sample_8.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_x2.h5 ./HVmodel/data/split_val/mix_sample_9.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_x2.h5 ./HVmodel/data/split_val/mix_sample_10.0_x2_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_0.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_1.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_2.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_3.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_4.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_5.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_6.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_7.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_8.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_9.0_x2_val_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_x2_val.h5 ./HVmodel/data/split_val/mix_sample_10.0_x2_val_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_x2.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_x2_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)\n",
    "    \n",
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_x2_val.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_x2_val_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation: +1, +3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_0.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_1.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_2.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_3.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_4.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_5.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_6.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_7.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_8.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_9.0_aug_1_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_aug_1.h5 ./HVmodel/data/split_val/mix_sample_10.0_aug_1_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_aug_1.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_aug_1_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_0.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_0.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_1.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_1.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_2.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_2.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_3.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_3.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_4.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_4.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_5.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_5.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_6.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_6.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_7.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_7.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_8.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_8.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_9.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_9.0_aug_3_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/split_val/mix_sample_10.0_aug_3.h5 ./HVmodel/data/split_val/mix_sample_10.0_aug_3_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_aug_3.h5'\n",
    "    output_path = f'./HVmodel/data/split_val/mix_sample_{i:.1f}_aug_3_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified SB region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_0.0.h5 ./HVmodel/data/new_SB/mix_sample_0.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_1.0.h5 ./HVmodel/data/new_SB/mix_sample_1.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_2.0.h5 ./HVmodel/data/new_SB/mix_sample_2.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_3.0.h5 ./HVmodel/data/new_SB/mix_sample_3.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_4.0.h5 ./HVmodel/data/new_SB/mix_sample_4.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_5.0.h5 ./HVmodel/data/new_SB/mix_sample_5.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_6.0.h5 ./HVmodel/data/new_SB/mix_sample_6.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_7.0.h5 ./HVmodel/data/new_SB/mix_sample_7.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_8.0.h5 ./HVmodel/data/new_SB/mix_sample_8.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_9.0.h5 ./HVmodel/data/new_SB/mix_sample_9.0_25x25.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_10.0.h5 ./HVmodel/data/new_SB/mix_sample_10.0_25x25.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new_SB/mix_sample_{i:.1f}.h5'\n",
    "    output_path = f'./HVmodel/data/new_SB/mix_sample_{i:.1f}_25x25.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_0.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_0.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_1.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_1.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_2.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_2.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_3.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_3.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_4.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_4.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_5.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_5.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_6.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_6.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_7.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_7.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_8.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_8.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_9.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_9.0_25x25_copy_1.npy 25 &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new_SB/mix_sample_10.0_copy_1.h5 ./HVmodel/data/new_SB/mix_sample_10.0_25x25_copy_1.npy 25 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new_SB/mix_sample_{i:.1f}_copy_1.h5'\n",
    "    output_path = f'./HVmodel/data/new_SB/mix_sample_{i:.1f}_25x25_copy_1.npy'\n",
    "    resolution = 25\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} {resolution} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0.npy SB_0.0_new \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0.npy SB_1.0_new \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0.npy SB_2.0_new \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0.npy SB_3.0_new \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0.npy SB_4.0_new \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0.npy SB_5.0_new \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0.npy SB_6.0_new \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0.npy SB_7.0_new \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0.npy SB_8.0_new \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0.npy SB_9.0_new \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0.npy SB_10.0_new \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_aug_1.npy SB_0.0_aug_1_new \"Sensitivity: 0.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_aug_1.npy SB_1.0_aug_1_new \"Sensitivity: 1.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_aug_1.npy SB_2.0_aug_1_new \"Sensitivity: 2.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_aug_1.npy SB_3.0_aug_1_new \"Sensitivity: 3.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_aug_1.npy SB_4.0_aug_1_new \"Sensitivity: 4.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_aug_1.npy SB_5.0_aug_1_new \"Sensitivity: 5.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_aug_1.npy SB_6.0_aug_1_new \"Sensitivity: 6.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_aug_1.npy SB_7.0_aug_1_new \"Sensitivity: 7.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_aug_1.npy SB_8.0_aug_1_new \"Sensitivity: 8.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_aug_1.npy SB_9.0_aug_1_new \"Sensitivity: 9.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_aug_1.npy SB_10.0_aug_1_new \"Sensitivity: 10.0, Augmentation: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_aug_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Augmentation: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_x2.npy SB_0.0_x2_new \"Sensitivity: 0.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_x2.npy SB_1.0_x2_new \"Sensitivity: 1.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_x2.npy SB_2.0_x2_new \"Sensitivity: 2.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_x2.npy SB_3.0_x2_new \"Sensitivity: 3.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_x2.npy SB_4.0_x2_new \"Sensitivity: 4.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_x2.npy SB_5.0_x2_new \"Sensitivity: 5.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_x2.npy SB_6.0_x2_new \"Sensitivity: 6.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_x2.npy SB_7.0_x2_new \"Sensitivity: 7.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_x2.npy SB_8.0_x2_new \"Sensitivity: 8.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_x2.npy SB_9.0_x2_new \"Sensitivity: 9.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_x2.npy SB_10.0_x2_new \"Sensitivity: 10.0, Luminosity: x2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_x2.npy'\n",
    "    model_name = f'SB_{i:.1f}_x2_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Luminosity: x2'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_11.0.npy SB_11.0_new \"Sensitivity: 11.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_12.0.npy SB_12.0_new \"Sensitivity: 12.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_13.0.npy SB_13.0_new \"Sensitivity: 13.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_14.0.npy SB_14.0_new \"Sensitivity: 14.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_15.0.npy SB_15.0_new \"Sensitivity: 15.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_16.0.npy SB_16.0_new \"Sensitivity: 16.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_17.0.npy SB_17.0_new \"Sensitivity: 17.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_18.0.npy SB_18.0_new \"Sensitivity: 18.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_19.0.npy SB_19.0_new \"Sensitivity: 19.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_20.0.npy SB_20.0_new \"Sensitivity: 20.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11, 21):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1.npy SB_0.0_copy_1_new \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1.npy SB_1.0_copy_1_new \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1.npy SB_2.0_copy_1_new \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1.npy SB_3.0_copy_1_new \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1.npy SB_4.0_copy_1_new \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1.npy SB_5.0_copy_1_new \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1.npy SB_6.0_copy_1_new \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1.npy SB_7.0_copy_1_new \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1.npy SB_8.0_copy_1_new \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1.npy SB_9.0_copy_1_new \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1.npy SB_10.0_copy_1_new \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_0.0_aug_1.npy SB_0.0_only_aug_1_new \"Sensitivity: 0.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_1.0_aug_1.npy SB_1.0_only_aug_1_new \"Sensitivity: 1.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_2.0_aug_1.npy SB_2.0_only_aug_1_new \"Sensitivity: 2.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_3.0_aug_1.npy SB_3.0_only_aug_1_new \"Sensitivity: 3.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_4.0_aug_1.npy SB_4.0_only_aug_1_new \"Sensitivity: 4.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_5.0_aug_1.npy SB_5.0_only_aug_1_new \"Sensitivity: 5.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_6.0_aug_1.npy SB_6.0_only_aug_1_new \"Sensitivity: 6.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_7.0_aug_1.npy SB_7.0_only_aug_1_new \"Sensitivity: 7.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_8.0_aug_1.npy SB_8.0_only_aug_1_new \"Sensitivity: 8.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_9.0_aug_1.npy SB_9.0_only_aug_1_new \"Sensitivity: 9.0, Only Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_10.0_aug_1.npy SB_10.0_only_aug_1_new \"Sensitivity: 10.0, Only Augmentation: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/aug_only/mix_sample_{i:.1f}_aug_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_only_aug_1_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Only Augmentation: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_0.0_aug_2.npy SB_0.0_only_aug_2_new \"Sensitivity: 0.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_1.0_aug_2.npy SB_1.0_only_aug_2_new \"Sensitivity: 1.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_2.0_aug_2.npy SB_2.0_only_aug_2_new \"Sensitivity: 2.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_3.0_aug_2.npy SB_3.0_only_aug_2_new \"Sensitivity: 3.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_4.0_aug_2.npy SB_4.0_only_aug_2_new \"Sensitivity: 4.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_5.0_aug_2.npy SB_5.0_only_aug_2_new \"Sensitivity: 5.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_6.0_aug_2.npy SB_6.0_only_aug_2_new \"Sensitivity: 6.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_7.0_aug_2.npy SB_7.0_only_aug_2_new \"Sensitivity: 7.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_8.0_aug_2.npy SB_8.0_only_aug_2_new \"Sensitivity: 8.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_9.0_aug_2.npy SB_9.0_only_aug_2_new \"Sensitivity: 9.0, Only Augmentation: 2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/aug_only/mix_sample_10.0_aug_2.npy SB_10.0_only_aug_2_new \"Sensitivity: 10.0, Only Augmentation: 2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/aug_only/mix_sample_{i:.1f}_aug_2.npy'\n",
    "    model_name = f'SB_{i:.1f}_only_aug_2_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Only Augmentation: 2'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change smearing scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_0.0_aug_1.npy SB_0.0_aug_1_std_02_new \"Sensitivity: 0.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_1.0_aug_1.npy SB_1.0_aug_1_std_02_new \"Sensitivity: 1.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_2.0_aug_1.npy SB_2.0_aug_1_std_02_new \"Sensitivity: 2.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_3.0_aug_1.npy SB_3.0_aug_1_std_02_new \"Sensitivity: 3.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_4.0_aug_1.npy SB_4.0_aug_1_std_02_new \"Sensitivity: 4.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_5.0_aug_1.npy SB_5.0_aug_1_std_02_new \"Sensitivity: 5.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_6.0_aug_1.npy SB_6.0_aug_1_std_02_new \"Sensitivity: 6.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_7.0_aug_1.npy SB_7.0_aug_1_std_02_new \"Sensitivity: 7.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_8.0_aug_1.npy SB_8.0_aug_1_std_02_new \"Sensitivity: 8.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_9.0_aug_1.npy SB_9.0_aug_1_std_02_new \"Sensitivity: 9.0, Augmentation: 1, Smearing: 0.2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_02/mix_sample_10.0_aug_1.npy SB_10.0_aug_1_std_02_new \"Sensitivity: 10.0, Augmentation: 1, Smearing: 0.2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/smearing_scale_02/mix_sample_{i:.1f}_aug_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_std_02_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Augmentation: 1, Smearing: 0.2'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_0.0_aug_1.npy SB_0.0_aug_1_std_05_new \"Sensitivity: 0.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_1.0_aug_1.npy SB_1.0_aug_1_std_05_new \"Sensitivity: 1.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_2.0_aug_1.npy SB_2.0_aug_1_std_05_new \"Sensitivity: 2.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_3.0_aug_1.npy SB_3.0_aug_1_std_05_new \"Sensitivity: 3.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_4.0_aug_1.npy SB_4.0_aug_1_std_05_new \"Sensitivity: 4.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_5.0_aug_1.npy SB_5.0_aug_1_std_05_new \"Sensitivity: 5.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_6.0_aug_1.npy SB_6.0_aug_1_std_05_new \"Sensitivity: 6.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_7.0_aug_1.npy SB_7.0_aug_1_std_05_new \"Sensitivity: 7.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_8.0_aug_1.npy SB_8.0_aug_1_std_05_new \"Sensitivity: 8.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_9.0_aug_1.npy SB_9.0_aug_1_std_05_new \"Sensitivity: 9.0, Augmentation: 1, Smearing: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/smearing_scale_05/mix_sample_10.0_aug_1.npy SB_10.0_aug_1_std_05_new \"Sensitivity: 10.0, Augmentation: 1, Smearing: 0.5\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/smearing_scale_05/mix_sample_{i:.1f}_aug_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_std_05_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Augmentation: 1, Smearing: 0.5'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1_no_shuffle.npy SB_0.0_copy_1_no_shuffle_new \"Sensitivity: 0.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1_no_shuffle.npy SB_1.0_copy_1_no_shuffle_new \"Sensitivity: 1.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1_no_shuffle.npy SB_2.0_copy_1_no_shuffle_new \"Sensitivity: 2.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1_no_shuffle.npy SB_3.0_copy_1_no_shuffle_new \"Sensitivity: 3.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1_no_shuffle.npy SB_4.0_copy_1_no_shuffle_new \"Sensitivity: 4.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1_no_shuffle.npy SB_5.0_copy_1_no_shuffle_new \"Sensitivity: 5.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1_no_shuffle.npy SB_6.0_copy_1_no_shuffle_new \"Sensitivity: 6.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1_no_shuffle.npy SB_7.0_copy_1_no_shuffle_new \"Sensitivity: 7.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1_no_shuffle.npy SB_8.0_copy_1_no_shuffle_new \"Sensitivity: 8.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1_no_shuffle.npy SB_9.0_copy_1_no_shuffle_new \"Sensitivity: 9.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1_no_shuffle.npy SB_10.0_copy_1_no_shuffle_new \"Sensitivity: 10.0, Copy: 1, No shuffle\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1_no_shuffle.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_no_shuffle_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 1, No shuffle'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1_no_shuffle.npy SB_0.0_copy_1_no_shuffle_code \"Sensitivity: 0.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1_no_shuffle.npy SB_1.0_copy_1_no_shuffle_code \"Sensitivity: 1.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1_no_shuffle.npy SB_2.0_copy_1_no_shuffle_code \"Sensitivity: 2.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1_no_shuffle.npy SB_3.0_copy_1_no_shuffle_code \"Sensitivity: 3.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1_no_shuffle.npy SB_4.0_copy_1_no_shuffle_code \"Sensitivity: 4.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1_no_shuffle.npy SB_5.0_copy_1_no_shuffle_code \"Sensitivity: 5.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1_no_shuffle.npy SB_6.0_copy_1_no_shuffle_code \"Sensitivity: 6.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1_no_shuffle.npy SB_7.0_copy_1_no_shuffle_code \"Sensitivity: 7.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1_no_shuffle.npy SB_8.0_copy_1_no_shuffle_code \"Sensitivity: 8.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1_no_shuffle.npy SB_9.0_copy_1_no_shuffle_code \"Sensitivity: 9.0, Copy: 1, No shuffle\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1_no_shuffle.npy SB_10.0_copy_1_no_shuffle_code \"Sensitivity: 10.0, Copy: 1, No shuffle\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1_no_shuffle.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_no_shuffle_code'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 1, No shuffle'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half + Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_half_copy_1.npy SB_0.0_half_copy_1 \"Sensitivity: 0.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_half_copy_1.npy SB_1.0_half_copy_1 \"Sensitivity: 1.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_half_copy_1.npy SB_2.0_half_copy_1 \"Sensitivity: 2.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_half_copy_1.npy SB_3.0_half_copy_1 \"Sensitivity: 3.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_half_copy_1.npy SB_4.0_half_copy_1 \"Sensitivity: 4.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_half_copy_1.npy SB_5.0_half_copy_1 \"Sensitivity: 5.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_half_copy_1.npy SB_6.0_half_copy_1 \"Sensitivity: 6.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_half_copy_1.npy SB_7.0_half_copy_1 \"Sensitivity: 7.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_half_copy_1.npy SB_8.0_half_copy_1 \"Sensitivity: 8.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_half_copy_1.npy SB_9.0_half_copy_1 \"Sensitivity: 9.0, Half, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_half_copy_1.npy SB_10.0_half_copy_1 \"Sensitivity: 10.0, Half, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_half_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_half_copy_1'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Half, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orginal + $x$ Copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_0.5.npy SB_0.0_copy_0.5 \"Sensitivity: 0.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_0.5.npy SB_1.0_copy_0.5 \"Sensitivity: 1.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_0.5.npy SB_2.0_copy_0.5 \"Sensitivity: 2.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_0.5.npy SB_3.0_copy_0.5 \"Sensitivity: 3.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_0.5.npy SB_4.0_copy_0.5 \"Sensitivity: 4.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_0.5.npy SB_5.0_copy_0.5 \"Sensitivity: 5.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_0.5.npy SB_6.0_copy_0.5 \"Sensitivity: 6.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_0.5.npy SB_7.0_copy_0.5 \"Sensitivity: 7.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_0.5.npy SB_8.0_copy_0.5 \"Sensitivity: 8.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_0.5.npy SB_9.0_copy_0.5 \"Sensitivity: 9.0, Copy: 0.5\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_0.5.npy SB_10.0_copy_0.5 \"Sensitivity: 10.0, Copy: 0.5\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_0.5.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_0.5'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 0.5'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_0.25.npy SB_0.0_copy_0.25 \"Sensitivity: 0.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_0.25.npy SB_1.0_copy_0.25 \"Sensitivity: 1.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_0.25.npy SB_2.0_copy_0.25 \"Sensitivity: 2.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_0.25.npy SB_3.0_copy_0.25 \"Sensitivity: 3.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_0.25.npy SB_4.0_copy_0.25 \"Sensitivity: 4.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_0.25.npy SB_5.0_copy_0.25 \"Sensitivity: 5.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_0.25.npy SB_6.0_copy_0.25 \"Sensitivity: 6.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_0.25.npy SB_7.0_copy_0.25 \"Sensitivity: 7.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_0.25.npy SB_8.0_copy_0.25 \"Sensitivity: 8.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_0.25.npy SB_9.0_copy_0.25 \"Sensitivity: 9.0, Copy: 0.25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_0.25.npy SB_10.0_copy_0.25 \"Sensitivity: 10.0, Copy: 0.25\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_0.25.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_0.25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 0.25'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_0.75.npy SB_0.0_copy_0.75 \"Sensitivity: 0.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_0.75.npy SB_1.0_copy_0.75 \"Sensitivity: 1.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_0.75.npy SB_2.0_copy_0.75 \"Sensitivity: 2.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_0.75.npy SB_3.0_copy_0.75 \"Sensitivity: 3.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_0.75.npy SB_4.0_copy_0.75 \"Sensitivity: 4.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_0.75.npy SB_5.0_copy_0.75 \"Sensitivity: 5.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_0.75.npy SB_6.0_copy_0.75 \"Sensitivity: 6.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_0.75.npy SB_7.0_copy_0.75 \"Sensitivity: 7.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_0.75.npy SB_8.0_copy_0.75 \"Sensitivity: 8.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_0.75.npy SB_9.0_copy_0.75 \"Sensitivity: 9.0, Copy: 0.75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_0.75.npy SB_10.0_copy_0.75 \"Sensitivity: 10.0, Copy: 0.75\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_0.75.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_0.75'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 0.75'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the swapping label procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0.npy SB_0.0_no_swap \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0.npy SB_1.0_no_swap \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0.npy SB_2.0_no_swap \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0.npy SB_3.0_no_swap \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0.npy SB_4.0_no_swap \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0.npy SB_5.0_no_swap \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0.npy SB_6.0_no_swap \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0.npy SB_7.0_no_swap \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0.npy SB_8.0_no_swap \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0.npy SB_9.0_no_swap \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0.npy SB_10.0_no_swap \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_no_swap'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_x2.npy SB_0.0_x2_no_swap \"Sensitivity: 0.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_x2.npy SB_1.0_x2_no_swap \"Sensitivity: 1.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_x2.npy SB_2.0_x2_no_swap \"Sensitivity: 2.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_x2.npy SB_3.0_x2_no_swap \"Sensitivity: 3.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_x2.npy SB_4.0_x2_no_swap \"Sensitivity: 4.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_x2.npy SB_5.0_x2_no_swap \"Sensitivity: 5.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_x2.npy SB_6.0_x2_no_swap \"Sensitivity: 6.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_x2.npy SB_7.0_x2_no_swap \"Sensitivity: 7.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_x2.npy SB_8.0_x2_no_swap \"Sensitivity: 8.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_x2.npy SB_9.0_x2_no_swap \"Sensitivity: 9.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_x2.npy SB_10.0_x2_no_swap \"Sensitivity: 10.0, Luminosity: x2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_x2.npy'\n",
    "    model_name = f'SB_{i:.1f}_x2_no_swap'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Luminosity: x2'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1.npy SB_0.0_copy_1_no_swap \"Sensitivity: 0.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1.npy SB_1.0_copy_1_no_swap \"Sensitivity: 1.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1.npy SB_2.0_copy_1_no_swap \"Sensitivity: 2.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1.npy SB_3.0_copy_1_no_swap \"Sensitivity: 3.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1.npy SB_4.0_copy_1_no_swap \"Sensitivity: 4.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1.npy SB_5.0_copy_1_no_swap \"Sensitivity: 5.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1.npy SB_6.0_copy_1_no_swap \"Sensitivity: 6.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1.npy SB_7.0_copy_1_no_swap \"Sensitivity: 7.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1.npy SB_8.0_copy_1_no_swap \"Sensitivity: 8.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1.npy SB_9.0_copy_1_no_swap \"Sensitivity: 9.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1.npy SB_10.0_copy_1_no_swap \"Sensitivity: 10.0, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_no_swap'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0.npy SB_0.0_2_image \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0.npy SB_1.0_2_image \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0.npy SB_2.0_2_image \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0.npy SB_3.0_2_image \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0.npy SB_4.0_2_image \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0.npy SB_5.0_2_image \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0.npy SB_6.0_2_image \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0.npy SB_7.0_2_image \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0.npy SB_8.0_2_image \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0.npy SB_9.0_2_image \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0.npy SB_10.0_2_image \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1.npy SB_0.0_copy_1_2_image \"Sensitivity: 0.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1.npy SB_1.0_copy_1_2_image \"Sensitivity: 1.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1.npy SB_2.0_copy_1_2_image \"Sensitivity: 2.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1.npy SB_3.0_copy_1_2_image \"Sensitivity: 3.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1.npy SB_4.0_copy_1_2_image \"Sensitivity: 4.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1.npy SB_5.0_copy_1_2_image \"Sensitivity: 5.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1.npy SB_6.0_copy_1_2_image \"Sensitivity: 6.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1.npy SB_7.0_copy_1_2_image \"Sensitivity: 7.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1.npy SB_8.0_copy_1_2_image \"Sensitivity: 8.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1.npy SB_9.0_copy_1_2_image \"Sensitivity: 9.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1.npy SB_10.0_copy_1_2_image \"Sensitivity: 10.0, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0.npy SB_0.0_2_image_tf2.5 \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0.npy SB_1.0_2_image_tf2.5 \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0.npy SB_2.0_2_image_tf2.5 \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0.npy SB_3.0_2_image_tf2.5 \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0.npy SB_4.0_2_image_tf2.5 \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0.npy SB_5.0_2_image_tf2.5 \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0.npy SB_6.0_2_image_tf2.5 \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0.npy SB_7.0_2_image_tf2.5 \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0.npy SB_8.0_2_image_tf2.5 \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0.npy SB_9.0_2_image_tf2.5 \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0.npy SB_10.0_2_image_tf2.5 \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image_tf2.5'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1.npy SB_0.0_copy_1_2_image_tf2.0 \"Sensitivity: 0.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1.npy SB_1.0_copy_1_2_image_tf2.0 \"Sensitivity: 1.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1.npy SB_2.0_copy_1_2_image_tf2.0 \"Sensitivity: 2.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1.npy SB_3.0_copy_1_2_image_tf2.0 \"Sensitivity: 3.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1.npy SB_4.0_copy_1_2_image_tf2.0 \"Sensitivity: 4.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1.npy SB_5.0_copy_1_2_image_tf2.0 \"Sensitivity: 5.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1.npy SB_6.0_copy_1_2_image_tf2.0 \"Sensitivity: 6.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1.npy SB_7.0_copy_1_2_image_tf2.0 \"Sensitivity: 7.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1.npy SB_8.0_copy_1_2_image_tf2.0 \"Sensitivity: 8.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1.npy SB_9.0_copy_1_2_image_tf2.0 \"Sensitivity: 9.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1.npy SB_10.0_copy_1_2_image_tf2.0 \"Sensitivity: 10.0, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image_tf2.0'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0.npy SB_0.0_2_image_patience_30 \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0.npy SB_1.0_2_image_patience_30 \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0.npy SB_2.0_2_image_patience_30 \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0.npy SB_3.0_2_image_patience_30 \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0.npy SB_4.0_2_image_patience_30 \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0.npy SB_5.0_2_image_patience_30 \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0.npy SB_6.0_2_image_patience_30 \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0.npy SB_7.0_2_image_patience_30 \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0.npy SB_8.0_2_image_patience_30 \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0.npy SB_9.0_2_image_patience_30 \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0.npy SB_10.0_2_image_patience_30 \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image_patience_30'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_copy_1.npy SB_0.0_copy_1_2_image_patience_30 \"Sensitivity: 0.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_copy_1.npy SB_1.0_copy_1_2_image_patience_30 \"Sensitivity: 1.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_copy_1.npy SB_2.0_copy_1_2_image_patience_30 \"Sensitivity: 2.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_copy_1.npy SB_3.0_copy_1_2_image_patience_30 \"Sensitivity: 3.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_copy_1.npy SB_4.0_copy_1_2_image_patience_30 \"Sensitivity: 4.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_copy_1.npy SB_5.0_copy_1_2_image_patience_30 \"Sensitivity: 5.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_copy_1.npy SB_6.0_copy_1_2_image_patience_30 \"Sensitivity: 6.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_copy_1.npy SB_7.0_copy_1_2_image_patience_30 \"Sensitivity: 7.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_copy_1.npy SB_8.0_copy_1_2_image_patience_30 \"Sensitivity: 8.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_copy_1.npy SB_9.0_copy_1_2_image_patience_30 \"Sensitivity: 9.0, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_copy_1.npy SB_10.0_copy_1_2_image_patience_30 \"Sensitivity: 10.0, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image_patience_30'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution $25 \\times 25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_25x25.npy SB_0.0_2_image_25x25 \"Sensitivity: 0.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_25x25.npy SB_1.0_2_image_25x25 \"Sensitivity: 1.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_25x25.npy SB_2.0_2_image_25x25 \"Sensitivity: 2.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_25x25.npy SB_3.0_2_image_25x25 \"Sensitivity: 3.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_25x25.npy SB_4.0_2_image_25x25 \"Sensitivity: 4.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_25x25.npy SB_5.0_2_image_25x25 \"Sensitivity: 5.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_25x25.npy SB_6.0_2_image_25x25 \"Sensitivity: 6.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_25x25.npy SB_7.0_2_image_25x25 \"Sensitivity: 7.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_25x25.npy SB_8.0_2_image_25x25 \"Sensitivity: 8.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_25x25.npy SB_9.0_2_image_25x25 \"Sensitivity: 9.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_25x25.npy SB_10.0_2_image_25x25 \"Sensitivity: 10.0, Resolution: 25x25\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_25x25_copy_1.npy SB_0.0_copy_1_2_image_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_25x25_copy_1.npy SB_1.0_copy_1_2_image_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_25x25_copy_1.npy SB_2.0_copy_1_2_image_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_25x25_copy_1.npy SB_3.0_copy_1_2_image_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_25x25_copy_1.npy SB_4.0_copy_1_2_image_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_25x25_copy_1.npy SB_5.0_copy_1_2_image_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_25x25_copy_1.npy SB_6.0_copy_1_2_image_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_25x25_copy_1.npy SB_7.0_copy_1_2_image_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_25x25_copy_1.npy SB_8.0_copy_1_2_image_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_25x25_copy_1.npy SB_9.0_copy_1_2_image_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_25x25_copy_1.npy SB_10.0_copy_1_2_image_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_25x25_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_25x25_aug_1.npy SB_0.0_aug_1_2_image_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_25x25_aug_1.npy SB_1.0_aug_1_2_image_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_25x25_aug_1.npy SB_2.0_aug_1_2_image_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_25x25_aug_1.npy SB_3.0_aug_1_2_image_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_25x25_aug_1.npy SB_4.0_aug_1_2_image_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_25x25_aug_1.npy SB_5.0_aug_1_2_image_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_25x25_aug_1.npy SB_6.0_aug_1_2_image_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_25x25_aug_1.npy SB_7.0_aug_1_2_image_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_25x25_aug_1.npy SB_8.0_aug_1_2_image_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_25x25_aug_1.npy SB_9.0_aug_1_2_image_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_25x25_aug_1.npy SB_10.0_aug_1_2_image_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Augmentation: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_25x25_aug_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_2_image_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Augmentation: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_25x25_x2.npy SB_0.0_x2_2_image_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_25x25_x2.npy SB_1.0_x2_2_image_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_25x25_x2.npy SB_2.0_x2_2_image_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_25x25_x2.npy SB_3.0_x2_2_image_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_25x25_x2.npy SB_4.0_x2_2_image_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_25x25_x2.npy SB_5.0_x2_2_image_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_25x25_x2.npy SB_6.0_x2_2_image_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_25x25_x2.npy SB_7.0_x2_2_image_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_25x25_x2.npy SB_8.0_x2_2_image_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_25x25_x2.npy SB_9.0_x2_2_image_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_25x25_x2.npy SB_10.0_x2_2_image_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Luminosity: x2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_25x25_x2.npy'\n",
    "    model_name = f'SB_{i:.1f}_x2_2_image_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Luminosity: x2'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Batchnormalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_25x25.npy SB_0.0_2_image_25x25_BN \"Sensitivity: 0.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_25x25.npy SB_1.0_2_image_25x25_BN \"Sensitivity: 1.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_25x25.npy SB_2.0_2_image_25x25_BN \"Sensitivity: 2.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_25x25.npy SB_3.0_2_image_25x25_BN \"Sensitivity: 3.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_25x25.npy SB_4.0_2_image_25x25_BN \"Sensitivity: 4.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_25x25.npy SB_5.0_2_image_25x25_BN \"Sensitivity: 5.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_25x25.npy SB_6.0_2_image_25x25_BN \"Sensitivity: 6.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_25x25.npy SB_7.0_2_image_25x25_BN \"Sensitivity: 7.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_25x25.npy SB_8.0_2_image_25x25_BN \"Sensitivity: 8.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_25x25.npy SB_9.0_2_image_25x25_BN \"Sensitivity: 9.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_25x25.npy SB_10.0_2_image_25x25_BN \"Sensitivity: 10.0, Resolution: 25x25\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image_25x25_BN'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_25x25_copy_1.npy SB_0.0_copy_1_2_image_25x25_BN \"Sensitivity: 0.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_25x25_copy_1.npy SB_1.0_copy_1_2_image_25x25_BN \"Sensitivity: 1.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_25x25_copy_1.npy SB_2.0_copy_1_2_image_25x25_BN \"Sensitivity: 2.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_25x25_copy_1.npy SB_3.0_copy_1_2_image_25x25_BN \"Sensitivity: 3.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_25x25_copy_1.npy SB_4.0_copy_1_2_image_25x25_BN \"Sensitivity: 4.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_25x25_copy_1.npy SB_5.0_copy_1_2_image_25x25_BN \"Sensitivity: 5.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_25x25_copy_1.npy SB_6.0_copy_1_2_image_25x25_BN \"Sensitivity: 6.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_25x25_copy_1.npy SB_7.0_copy_1_2_image_25x25_BN \"Sensitivity: 7.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_25x25_copy_1.npy SB_8.0_copy_1_2_image_25x25_BN \"Sensitivity: 8.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_25x25_copy_1.npy SB_9.0_copy_1_2_image_25x25_BN \"Sensitivity: 9.0, Resolution: 25x25, Copy: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_25x25_copy_1.npy SB_10.0_copy_1_2_image_25x25_BN \"Sensitivity: 10.0, Resolution: 25x25, Copy: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_25x25_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image_25x25_BN'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Copy: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SB region: 4400-4700 5500-5800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_0.0_25x25.npy SB_0.0_2_image_25x25_new_SB \"Sensitivity: 0.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_1.0_25x25.npy SB_1.0_2_image_25x25_new_SB \"Sensitivity: 1.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_2.0_25x25.npy SB_2.0_2_image_25x25_new_SB \"Sensitivity: 2.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_3.0_25x25.npy SB_3.0_2_image_25x25_new_SB \"Sensitivity: 3.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_4.0_25x25.npy SB_4.0_2_image_25x25_new_SB \"Sensitivity: 4.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_5.0_25x25.npy SB_5.0_2_image_25x25_new_SB \"Sensitivity: 5.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_6.0_25x25.npy SB_6.0_2_image_25x25_new_SB \"Sensitivity: 6.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_7.0_25x25.npy SB_7.0_2_image_25x25_new_SB \"Sensitivity: 7.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_8.0_25x25.npy SB_8.0_2_image_25x25_new_SB \"Sensitivity: 8.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_9.0_25x25.npy SB_9.0_2_image_25x25_new_SB \"Sensitivity: 9.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_10.0_25x25.npy SB_10.0_2_image_25x25_new_SB \"Sensitivity: 10.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new_SB/mix_sample_{i:.1f}_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image_25x25_new_SB'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, SB region: 4400-4700, 5500-5800'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_0.0_25x25_copy_1.npy SB_0.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 0.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_1.0_25x25_copy_1.npy SB_1.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 1.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_2.0_25x25_copy_1.npy SB_2.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 2.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_3.0_25x25_copy_1.npy SB_3.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 3.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_4.0_25x25_copy_1.npy SB_4.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 4.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_5.0_25x25_copy_1.npy SB_5.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 5.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_6.0_25x25_copy_1.npy SB_6.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 6.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_7.0_25x25_copy_1.npy SB_7.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 7.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_8.0_25x25_copy_1.npy SB_8.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 8.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_9.0_25x25_copy_1.npy SB_9.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 9.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new_SB/mix_sample_10.0_25x25_copy_1.npy SB_10.0_copy_1_2_image_25x25_new_SB \"Sensitivity: 10.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new_SB/mix_sample_{i:.1f}_25x25_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image_25x25_new_SB'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_25x25.npy SB_0.0_2_image_25x25_split_val \"Sensitivity: 0.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_25x25.npy SB_1.0_2_image_25x25_split_val \"Sensitivity: 1.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_25x25.npy SB_2.0_2_image_25x25_split_val \"Sensitivity: 2.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_25x25.npy SB_3.0_2_image_25x25_split_val \"Sensitivity: 3.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_25x25.npy SB_4.0_2_image_25x25_split_val \"Sensitivity: 4.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_25x25.npy SB_5.0_2_image_25x25_split_val \"Sensitivity: 5.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_25x25.npy SB_6.0_2_image_25x25_split_val \"Sensitivity: 6.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_25x25.npy SB_7.0_2_image_25x25_split_val \"Sensitivity: 7.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_25x25.npy SB_8.0_2_image_25x25_split_val \"Sensitivity: 8.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_25x25.npy SB_9.0_2_image_25x25_split_val \"Sensitivity: 9.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_25x25.npy SB_10.0_2_image_25x25_split_val \"Sensitivity: 10.0, Resolution: 25x25, SB region: 4400-4700, 5500-5800\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_2_image_25x25_split_val'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, SB region: 4400-4700, 5500-5800'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_25x25_copy_1.npy SB_0.0_copy_1_2_image_25x25_split_val \"Sensitivity: 0.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_25x25_copy_1.npy SB_1.0_copy_1_2_image_25x25_split_val \"Sensitivity: 1.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_25x25_copy_1.npy SB_2.0_copy_1_2_image_25x25_split_val \"Sensitivity: 2.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_25x25_copy_1.npy SB_3.0_copy_1_2_image_25x25_split_val \"Sensitivity: 3.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_25x25_copy_1.npy SB_4.0_copy_1_2_image_25x25_split_val \"Sensitivity: 4.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_25x25_copy_1.npy SB_5.0_copy_1_2_image_25x25_split_val \"Sensitivity: 5.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_25x25_copy_1.npy SB_6.0_copy_1_2_image_25x25_split_val \"Sensitivity: 6.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_25x25_copy_1.npy SB_7.0_copy_1_2_image_25x25_split_val \"Sensitivity: 7.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_25x25_copy_1.npy SB_8.0_copy_1_2_image_25x25_split_val \"Sensitivity: 8.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_25x25_copy_1.npy SB_9.0_copy_1_2_image_25x25_split_val \"Sensitivity: 9.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_25x25_copy_1.npy SB_10.0_copy_1_2_image_25x25_split_val \"Sensitivity: 10.0, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_25x25_copy_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_2_image_25x25_split_val'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Copy: 1, SB region: 4400-4700, 5500-5800'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New data process flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original mix dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_0.0_75x75 \"Sensitivity: 0.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_1.0_75x75 \"Sensitivity: 1.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_2.0_75x75 \"Sensitivity: 2.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_3.0_75x75 \"Sensitivity: 3.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_4.0_75x75 \"Sensitivity: 4.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_5.0_75x75 \"Sensitivity: 5.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_6.0_75x75 \"Sensitivity: 6.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_7.0_75x75 \"Sensitivity: 7.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_8.0_75x75 \"Sensitivity: 8.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_9.0_75x75 \"Sensitivity: 9.0, Resolution: 75x75\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_10.0_75x75 \"Sensitivity: 10.0, Resolution: 75x75\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_75x75.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_75x75.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy'\n",
    "    model_name = f'SB_{i:.1f}_75x75'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 75x75'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_0.0_25x25 \"Sensitivity: 0.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_1.0_25x25 \"Sensitivity: 1.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_2.0_25x25 \"Sensitivity: 2.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_3.0_25x25 \"Sensitivity: 3.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_4.0_25x25 \"Sensitivity: 4.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_5.0_25x25 \"Sensitivity: 5.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_6.0_25x25 \"Sensitivity: 6.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_7.0_25x25 \"Sensitivity: 7.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_8.0_25x25 \"Sensitivity: 8.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_9.0_25x25 \"Sensitivity: 9.0, Resolution: 25x25\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_10.0_25x25 \"Sensitivity: 10.0, Resolution: 25x25\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_25x25.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_25x25.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy: +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_0.0_copy_1_75x75 \"Sensitivity: 0.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_1.0_copy_1_75x75 \"Sensitivity: 1.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_2.0_copy_1_75x75 \"Sensitivity: 2.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_3.0_copy_1_75x75 \"Sensitivity: 3.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_4.0_copy_1_75x75 \"Sensitivity: 4.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_5.0_copy_1_75x75 \"Sensitivity: 5.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_6.0_copy_1_75x75 \"Sensitivity: 6.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_7.0_copy_1_75x75 \"Sensitivity: 7.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_8.0_copy_1_75x75 \"Sensitivity: 8.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_9.0_copy_1_75x75 \"Sensitivity: 9.0, Resolution: 75x75, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_copy_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_10.0_copy_1_75x75 \"Sensitivity: 10.0, Resolution: 75x75, Copy: +1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_copy_1_75x75.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_75x75.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_75x75'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 75x75, Copy: +1'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_0.0_copy_1_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_1.0_copy_1_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_2.0_copy_1_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_3.0_copy_1_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_4.0_copy_1_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_5.0_copy_1_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_6.0_copy_1_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_7.0_copy_1_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_8.0_copy_1_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_9.0_copy_1_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Copy: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_copy_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_10.0_copy_1_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Copy: +1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_copy_1_25x25.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_25x25.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_copy_1_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Copy: +1'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation: +1, +3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_0.0_aug_1_75x75 \"Sensitivity: 0.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_1.0_aug_1_75x75 \"Sensitivity: 1.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_2.0_aug_1_75x75 \"Sensitivity: 2.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_3.0_aug_1_75x75 \"Sensitivity: 3.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_4.0_aug_1_75x75 \"Sensitivity: 4.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_5.0_aug_1_75x75 \"Sensitivity: 5.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_6.0_aug_1_75x75 \"Sensitivity: 6.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_7.0_aug_1_75x75 \"Sensitivity: 7.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_8.0_aug_1_75x75 \"Sensitivity: 8.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_9.0_aug_1_75x75 \"Sensitivity: 9.0, Resolution: 75x75, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_aug_1_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_10.0_aug_1_75x75 \"Sensitivity: 10.0, Resolution: 75x75, Augmentation: +1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_aug_1_75x75.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_75x75.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_75x75'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 75x75, Augmentation: +1'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_0.0_aug_1_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_1.0_aug_1_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_2.0_aug_1_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_3.0_aug_1_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_4.0_aug_1_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_5.0_aug_1_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_6.0_aug_1_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_7.0_aug_1_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_8.0_aug_1_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_9.0_aug_1_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Augmentation: +1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_aug_1_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_10.0_aug_1_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Augmentation: +1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_aug_1_25x25.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_25x25.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Augmentation: +1'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_0.0_aug_3_75x75 \"Sensitivity: 0.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_1.0_aug_3_75x75 \"Sensitivity: 1.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_2.0_aug_3_75x75 \"Sensitivity: 2.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_3.0_aug_3_75x75 \"Sensitivity: 3.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_4.0_aug_3_75x75 \"Sensitivity: 4.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_5.0_aug_3_75x75 \"Sensitivity: 5.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_6.0_aug_3_75x75 \"Sensitivity: 6.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_7.0_aug_3_75x75 \"Sensitivity: 7.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_8.0_aug_3_75x75 \"Sensitivity: 8.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_9.0_aug_3_75x75 \"Sensitivity: 9.0, Resolution: 75x75, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_aug_3_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_75x75.npy ../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy SB_10.0_aug_3_75x75 \"Sensitivity: 10.0, Resolution: 75x75, Augmentation: +3\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_aug_3_75x75.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_75x75.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_75x75.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_3_75x75'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 75x75, Augmentation: +3'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_0.0_aug_3_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_1.0_aug_3_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_2.0_aug_3_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_3.0_aug_3_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_4.0_aug_3_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_5.0_aug_3_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_6.0_aug_3_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_7.0_aug_3_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_8.0_aug_3_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_9.0_aug_3_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Augmentation: +3\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_aug_3_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_10.0_aug_3_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Augmentation: +3\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_aug_3_25x25.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_val_25x25.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_3_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Augmentation: +3'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luminosity: $\\times 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_0.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_0.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_0.0_x2_25x25 \"Sensitivity: 0.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_1.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_1.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_1.0_x2_25x25 \"Sensitivity: 1.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_2.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_2.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_2.0_x2_25x25 \"Sensitivity: 2.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_3.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_3.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_3.0_x2_25x25 \"Sensitivity: 3.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_4.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_4.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_4.0_x2_25x25 \"Sensitivity: 4.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_5.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_5.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_5.0_x2_25x25 \"Sensitivity: 5.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_6.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_6.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_6.0_x2_25x25 \"Sensitivity: 6.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_7.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_7.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_7.0_x2_25x25 \"Sensitivity: 7.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_8.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_8.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_8.0_x2_25x25 \"Sensitivity: 8.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_9.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_9.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_9.0_x2_25x25 \"Sensitivity: 9.0, Resolution: 25x25, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/split_val/mix_sample_10.0_x2_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_10.0_x2_val_25x25.npy ../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy SB_10.0_x2_25x25 \"Sensitivity: 10.0, Resolution: 25x25, Luminosity: x2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_x2_25x25.npy'\n",
    "    val_file = f'../Sample/HVmodel/data/split_val/mix_sample_{i:.1f}_x2_val_25x25.npy'\n",
    "    true_label_file = '../Sample/HVmodel/data/split_val/mix_sample_test_25x25.npy'\n",
    "    model_name = f'SB_{i:.1f}_x2_25x25'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Resolution: 25x25, Luminosity: x2'\n",
    "    cmd = f'python train_CNN.py {train_file} {val_file} {true_label_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
