{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample preparation\n",
    "* Generate .root file\n",
    "* Select the events and save jet constitutent information to .h5 file\n",
    "    - Sample/from_root_to_h5.py\n",
    "* Generate mixed sample\n",
    "* Data augmentation\n",
    "    - optional\n",
    "    - Sample/physical_augmentation_h5.ipynb\n",
    "* From .h5 file generate jet image and save in .npy file\n",
    "    - Sample/from_h5_to_npy.py\n",
    "2. CNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From root to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_root_to_h5.py ../../Software/pythia8307/HVmodel/test_100k.root ./HVmodel/data/new/signal.h5 1 &\n",
      "python from_root_to_h5.py ../../Software/pythia8307/HVmodel/test_100k-2.root ./HVmodel/data/new/signal-test.h5 1 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_03/tag_1_delphes_events.root ./HVmodel/data/new/background_03.h5 0 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_04/tag_1_delphes_events.root ./HVmodel/data/new/background-test.h5 0 &\n",
      "python from_root_to_h5.py ./ppjj/Events/run_05/tag_1_delphes_events.root ./HVmodel/data/new/background_05.h5 0 &\n"
     ]
    }
   ],
   "source": [
    "root_path = '../../Software/pythia8307/HVmodel/test_100k.root'\n",
    "output_path = './HVmodel/data/new/signal.h5'\n",
    "sample_type = 1\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = '../../Software/pythia8307/HVmodel/test_100k-2.root'\n",
    "output_path = './HVmodel/data/new/signal-test.h5'\n",
    "sample_type = 1\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_03/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/new/background_03.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_04/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/new/background-test.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)\n",
    "\n",
    "root_path = './ppjj/Events/run_05/tag_1_delphes_events.root'\n",
    "output_path = './HVmodel/data/new/background_05.h5'\n",
    "sample_type = 0\n",
    "\n",
    "cmd = f'python from_root_to_h5.py {root_path} {output_path} {sample_type} &'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate mixed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ROOT\n",
    "\n",
    "ROOT.gROOT.ProcessLine('.include /usr/local/Delphes-3.4.2/')\n",
    "ROOT.gROOT.ProcessLine('.include /usr/local/Delphes-3.4.2/external/')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/classes/DelphesClasses.h\"')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/external/ExRootAnalysis/ExRootTreeReader.h\"')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/external/ExRootAnalysis/ExRootConfReader.h\"')\n",
    "ROOT.gInterpreter.Declare('#include \"/usr/local/Delphes-3.4.2/external/ExRootAnalysis/ExRootTask.h\"')\n",
    "ROOT.gSystem.Load(\"/usr/local/Delphes-3.4.2/install/lib/libDelphes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mjets(*arg):\n",
    "    # arg: list of jets\n",
    "    # return: invariant mass of jets\n",
    "    e_tot, px_tot, py_tot, pz_tot = 0, 0, 0, 0\n",
    "    \n",
    "    for jet in arg:\n",
    "        pt, eta, phi, m = jet[0], jet[1], jet[2], jet[3]\n",
    "        \n",
    "        px, py, pz = pt*np.cos(phi), pt*np.sin(phi), pt*np.sinh(eta)\n",
    "        e = np.sqrt(m**2 + px**2 + py**2 + pz**2)\n",
    "        \n",
    "        px_tot += px\n",
    "        py_tot += py\n",
    "        pz_tot += pz\n",
    "        e_tot += e\n",
    "    \n",
    "    return np.sqrt(e_tot**2 - px_tot**2 - py_tot**2 - pz_tot**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HV_selection(tree):\n",
    "    # Hidden Valley model selection\n",
    "    # 1. 2 jets\n",
    "    # 2. pT > 750 GeV\n",
    "    # 3. |eta| < 2.0\n",
    "\n",
    "    n_event_count = 0\n",
    "    n_jet_count = 0\n",
    "    jet_pt_count = 0\n",
    "    jet_eta_count = 0\n",
    "    mjjs = []\n",
    "\n",
    "    pt = [[],[]]\n",
    "\n",
    "    for event_id, event in tqdm(enumerate(tree)):\n",
    "        n_event_count += 1\n",
    "\n",
    "        if event.Jet_size < 2:\n",
    "            continue\n",
    "        n_jet_count += 1\n",
    "\n",
    "        pt[0].append(event.Jet[0].PT)\n",
    "        pt[1].append(event.Jet[1].PT)\n",
    "        if event.Jet[1].PT < 750:\n",
    "            continue\n",
    "        jet_pt_count += 1\n",
    "\n",
    "        if abs(event.Jet[0].Eta) > 2.0 or abs(event.Jet[1].Eta) > 2.0:\n",
    "            continue\n",
    "        jet_eta_count += 1\n",
    "\n",
    "        jets = [[event.Jet[i].PT, event.Jet[i].Eta, event.Jet[i].Phi, event.Jet[i].Mass] for i in range(2)]\n",
    "        mjj = Mjets(*jets)\n",
    "        mjjs.append(mjj)\n",
    "\n",
    "        if mjj < 4300 or mjj > 5900:\n",
    "            continue\n",
    "\n",
    "\n",
    "    mjjs = np.array(mjjs)\n",
    "    SR_count = ((mjjs > 4700) & (mjjs < 5500)).sum()\n",
    "    SB_count = (((mjjs > 4300) & (mjjs < 4700)) | ((mjjs > 5500) & (mjjs < 5900))).sum()\n",
    "\n",
    "    cutflow_number = {\n",
    "        'Total': n_event_count,\n",
    "        'n jet cut': n_jet_count,\n",
    "        'jet pt cut': jet_pt_count,\n",
    "        'jet eta cut': jet_eta_count,\n",
    "        'Signal region': SR_count,\n",
    "        'Sideband region': SB_count,\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        'mjj': mjjs,\n",
    "        'pt': np.array(pt),\n",
    "        'cutflow_number': cutflow_number,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [02:41, 620.00it/s]\n"
     ]
    }
   ],
   "source": [
    "root_file = '../../Software/pythia8307/HVmodel/test_100k.root'\n",
    "f = ROOT.TFile(root_file)\n",
    "tree_s = f.Get(\"Delphes\")\n",
    "\n",
    "results_s = HV_selection(tree_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [02:18, 720.06it/s]\n",
      "Warning in <TStreamerInfo::BuildCheck>: \n",
      "   The StreamerInfo for version 2 of class GenParticle read from the file ./ppjj/Events/run_02/tag_1_delphes_events.root\n",
      "   has a different checksum than the previously loaded StreamerInfo.\n",
      "   Reading objects of type GenParticle from the file ./ppjj/Events/run_02/tag_1_delphes_events.root \n",
      "   (and potentially other files) might not work correctly.\n",
      "   Most likely the version number of the class was not properly\n",
      "   updated [See ClassDef(GenParticle,2)].\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float T; //number\n",
      "vs\n",
      "   float CtgTheta; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float X; //number\n",
      "vs\n",
      "   float D0; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float Y; //number\n",
      "vs\n",
      "   float DZ; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the on-file layout version 2 of class 'GenParticle' differs from \n",
      "the in-memory layout version 2:\n",
      "   float Z; //number\n",
      "vs\n",
      "   float T; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 2 of class 'GenParticle' is missing from \n",
      "the on-file layout version 2:\n",
      "   float X; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 2 of class 'GenParticle' is missing from \n",
      "the on-file layout version 2:\n",
      "   float Y; //number\n",
      "Warning in <TStreamerInfo::CompareContent>: The following data member of\n",
      "the in-memory layout version 2 of class 'GenParticle' is missing from \n",
      "the on-file layout version 2:\n",
      "   float Z; //number\n"
     ]
    }
   ],
   "source": [
    "root_file = './ppjj/Events/run_02/tag_1_delphes_events.root'\n",
    "f = ROOT.TFile(root_file)\n",
    "tree_b = f.Get(\"Delphes\")\n",
    "\n",
    "results_b = HV_selection(tree_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./HVmodel/data/selection_results_s.npy', results_s)\n",
    "np.save('./HVmodel/data/selection_results_b.npy', results_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path):\n",
    "    # path: run path\n",
    "    name = os.path.split(path)[1]\n",
    "\n",
    "    with open(os.path.join(path, f'{name}_tag_1_banner.txt')) as f:\n",
    "        for line in f.readlines():\n",
    "                \n",
    "            #  Integrated weight (pb)  :       0.020257\n",
    "            match = re.match('#  Integrated weight \\(pb\\)  : +(\\d+\\.\\d+)', line)\n",
    "            if match:\n",
    "                # unit: fb\n",
    "                cross_section = float(match.group(1)) * 1000\n",
    "            # #  Number of Events        :       100000\n",
    "            match = re.match('#  Number of Events        :       (\\d+)', line)\n",
    "            if match:\n",
    "                # unit: fb\n",
    "                nevent = int(match.group(1))\n",
    "    \n",
    "    return cross_section, nevent\n",
    "\n",
    "def get_dataset_keys(f):\n",
    "    keys = []\n",
    "    f.visit(lambda key : keys.append(key) if isinstance(f[key], h5py.Dataset) else None)\n",
    "    return keys\n",
    "\n",
    "def create_dataset(f, nevent, MAX_JETS):\n",
    "\n",
    "    f.create_dataset('J1/MASK', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='|b1')\n",
    "    f.create_dataset('J1/pt', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J1/eta', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J1/phi', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "\n",
    "    f.create_dataset('J2/MASK', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='|b1')\n",
    "    f.create_dataset('J2/pt', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J2/eta', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "    f.create_dataset('J2/phi', (nevent, MAX_JETS), maxshape=(None, MAX_JETS), dtype='<f4')\n",
    "\n",
    "    f.create_dataset('EVENT/Mjj', (nevent,), maxshape=(None,), dtype='<f4')\n",
    "    f.create_dataset('EVENT/signal', (nevent,), maxshape=(None,), dtype='<i8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_s = np.load('./Sample/HVmodel/data/selection_results_s.npy', allow_pickle=True).item()\n",
    "results_b = np.load('./Sample/HVmodel/data/selection_results_b.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6837.392481 1000000\n",
      "Background cross section, SR: 136.13 fb, SB: 211.28 fb\n",
      "Background sample size: SR: 18922.4, SB: 29367.3\n",
      "137.55877041193227 39.606895448991104\n"
     ]
    }
   ],
   "source": [
    "# Total cross section and number of events\n",
    "xection, tot_event = get_info('./Sample/ppjj/Events/run_03')\n",
    "print(xection, tot_event)\n",
    "\n",
    "# cross section in signal region and sideband region\n",
    "cross_section_SR = results_b['cutflow_number']['Signal region'] / results_b['cutflow_number']['Total'] * xection\n",
    "cross_section_SB = results_b['cutflow_number']['Sideband region'] / results_b['cutflow_number']['Total'] * xection\n",
    "print(f'Background cross section, SR: {cross_section_SR:.2f} fb, SB: {cross_section_SB:.2f} fb')\n",
    "\n",
    "# number of background events in signal region and sideband region\n",
    "L = 139 * 1\n",
    "n_SR_B = cross_section_SR * L\n",
    "n_SB_B = cross_section_SB * L\n",
    "\n",
    "print(f'Background sample size: SR: {n_SR_B:.1f}, SB: {n_SB_B:.1f}')\n",
    "\n",
    "sensitivity = 1\n",
    "n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "print(n_SR_S, n_SB_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path):\n",
    "    # n_sig_1: number of signal events in mixing sample 1 (Signal region)\n",
    "    # n_sig_2: number of signal events in mixing sample 2 (Sideband region)\n",
    "\n",
    "    nevent = n_sig_1 + n_sig_2 + n_bkg_1 + n_bkg_2\n",
    "    with h5py.File(output_path, 'w') as f_out:\n",
    "        MAX_JETS = 300 \n",
    "        create_dataset(f_out, nevent, MAX_JETS)\n",
    "\n",
    "        keys = get_dataset_keys(f_out)\n",
    "        with h5py.File(sig_path, 'r') as f_sig, h5py.File(bkg_path, 'r') as f_bkg:  \n",
    "            mjj_s = f_sig['EVENT/Mjj'][:]\n",
    "            mjj_b = f_bkg['EVENT/Mjj'][:]\n",
    "            SR_range_s = (mjj_s > 4700) & (mjj_s < 5500)\n",
    "            SB_range_s = ((mjj_s > 4300) & (mjj_s < 4700)) | ((mjj_s > 5500) & (mjj_s < 5900))\n",
    "            SR_range_b = (mjj_b > 4700) & (mjj_b < 5500)\n",
    "            SB_range_b = ((mjj_b > 4300) & (mjj_b < 4700)) | ((mjj_b > 5500) & (mjj_b < 5900))\n",
    "\n",
    "            for key in keys:\n",
    "                f_out[key][:n_sig_1] = f_sig[key][:][SR_range_s][:n_sig_1]\n",
    "                f_out[key][n_sig_1:n_sig_1+n_bkg_1] = f_bkg[key][:][SR_range_b][:n_bkg_1]\n",
    "                f_out[key][n_sig_1+n_bkg_1:n_sig_1+n_bkg_1+n_sig_2] = f_sig[key][:][SB_range_s][:n_sig_2]\n",
    "                f_out[key][n_sig_1+n_bkg_1+n_sig_2:] = f_bkg[key][:][SB_range_b][:n_bkg_2]\n",
    "\n",
    "        f_out['EVENT/signal'][:n_sig_1+n_bkg_1] = 1\n",
    "        f_out['EVENT/signal'][n_sig_1+n_bkg_1:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path = './Sample/HVmodel/data/new/signal.h5'\n",
    "bkg_path = './Sample/HVmodel/data/new/background_03.h5'\n",
    "n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S), int(n_SB_S), int(n_SR_B), int(n_SB_B)\n",
    "output_path = './Sample/HVmodel/data/new/mix_sample.h5'\n",
    "create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path = './Sample/HVmodel/data/new/signal.h5'\n",
    "bkg_path = './Sample/HVmodel/data/new/background_03.h5'\n",
    "\n",
    "for i in range(11):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S), int(n_SB_S), int(n_SR_B), int(n_SB_B)\n",
    "    output_path = f'./Sample/HVmodel/data/new/mix_sample_{i:.1f}.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path = './Sample/HVmodel/data/new/signal.h5'\n",
    "bkg_path = './Sample/HVmodel/data/new/background_03_merged.h5'\n",
    "\n",
    "for i in range(11):\n",
    "    sensitivity = i\n",
    "    n_SR_S = sensitivity * np.sqrt(n_SR_B)\n",
    "    n_SB_S = n_SR_S * results_s['cutflow_number']['Sideband region'] / results_s['cutflow_number']['Signal region']\n",
    "\n",
    "    n_sig_1, n_sig_2, n_bkg_1, n_bkg_2 = int(n_SR_S), int(n_SB_S), int(n_SR_B), int(n_SB_B)\n",
    "    output_path = f'./Sample/HVmodel/data/new/mix_sample_{i:.1f}_x2.h5'\n",
    "    create_mix_sample_from_numbers(sig_path, bkg_path, n_sig_1, n_sig_2, n_bkg_1, n_bkg_2, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_path, bkg_path  = './Sample/HVmodel/data/new/signal-test.h5', './Sample/HVmodel/data/new/background-test.h5'\n",
    "n_sig_1, n_bkg_1 = 10000, 10000\n",
    "nevent = n_sig_1 + n_bkg_1\n",
    "\n",
    "with h5py.File('./Sample/HVmodel/data/new/mix_sample_testing.h5', 'w') as f_out:\n",
    "    \n",
    "    MAX_JETS = 300 \n",
    "    create_dataset(f_out, nevent, MAX_JETS)\n",
    "\n",
    "    keys = get_dataset_keys(f_out)\n",
    "    with h5py.File(sig_path, 'r') as f_sig, h5py.File(bkg_path, 'r') as f_bkg:  \n",
    "        mjj_s = f_sig['EVENT/Mjj'][:]\n",
    "        mjj_b = f_bkg['EVENT/Mjj'][:]\n",
    "        SR_range_s = (mjj_s > 4700) & (mjj_s < 5500)\n",
    "        SR_range_b = (mjj_b > 4700) & (mjj_b < 5500)\n",
    "\n",
    "        for key in keys:\n",
    "            f_out[key][:n_sig_1] = f_sig[key][:][SR_range_s][:n_sig_1]\n",
    "            f_out[key][n_sig_1:] = f_bkg[key][:][SR_range_b][:n_bkg_1]\n",
    "\n",
    "    f_out['EVENT/signal'][:n_sig_1] = 1\n",
    "    f_out['EVENT/signal'][n_sig_1:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Sample/physical_augmentation_h5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From h5 to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_testing.h5 ./HVmodel/data/new/mix_sample_testing.npy &\n"
     ]
    }
   ],
   "source": [
    "h5_path = f'./HVmodel/data/new/mix_sample_testing.h5'\n",
    "output_path = f'./HVmodel/data/new/mix_sample_testing.npy'\n",
    "\n",
    "cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_0.0.h5 ./HVmodel/data/new/mix_sample_0.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_1.0.h5 ./HVmodel/data/new/mix_sample_1.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_2.0.h5 ./HVmodel/data/new/mix_sample_2.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_3.0.h5 ./HVmodel/data/new/mix_sample_3.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_4.0.h5 ./HVmodel/data/new/mix_sample_4.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_5.0.h5 ./HVmodel/data/new/mix_sample_5.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_6.0.h5 ./HVmodel/data/new/mix_sample_6.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_7.0.h5 ./HVmodel/data/new/mix_sample_7.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_8.0.h5 ./HVmodel/data/new/mix_sample_8.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_9.0.h5 ./HVmodel/data/new/mix_sample_9.0.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_10.0.h5 ./HVmodel/data/new/mix_sample_10.0.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_0.0_aug_1.h5 ./HVmodel/data/new/mix_sample_0.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_1.0_aug_1.h5 ./HVmodel/data/new/mix_sample_1.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_2.0_aug_1.h5 ./HVmodel/data/new/mix_sample_2.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_3.0_aug_1.h5 ./HVmodel/data/new/mix_sample_3.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_4.0_aug_1.h5 ./HVmodel/data/new/mix_sample_4.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_5.0_aug_1.h5 ./HVmodel/data/new/mix_sample_5.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_6.0_aug_1.h5 ./HVmodel/data/new/mix_sample_6.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_7.0_aug_1.h5 ./HVmodel/data/new/mix_sample_7.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_8.0_aug_1.h5 ./HVmodel/data/new/mix_sample_8.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_9.0_aug_1.h5 ./HVmodel/data/new/mix_sample_9.0_aug_1.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_10.0_aug_1.h5 ./HVmodel/data/new/mix_sample_10.0_aug_1.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_aug_1.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_aug_1.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_0.0_x2.h5 ./HVmodel/data/new/mix_sample_0.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_1.0_x2.h5 ./HVmodel/data/new/mix_sample_1.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_2.0_x2.h5 ./HVmodel/data/new/mix_sample_2.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_3.0_x2.h5 ./HVmodel/data/new/mix_sample_3.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_4.0_x2.h5 ./HVmodel/data/new/mix_sample_4.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_5.0_x2.h5 ./HVmodel/data/new/mix_sample_5.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_6.0_x2.h5 ./HVmodel/data/new/mix_sample_6.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_7.0_x2.h5 ./HVmodel/data/new/mix_sample_7.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_8.0_x2.h5 ./HVmodel/data/new/mix_sample_8.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_9.0_x2.h5 ./HVmodel/data/new/mix_sample_9.0_x2.npy &\n",
      "python from_h5_to_npy.py ./HVmodel/data/new/mix_sample_10.0_x2.h5 ./HVmodel/data/new/mix_sample_10.0_x2.npy &\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    h5_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_x2.h5'\n",
    "    output_path = f'./HVmodel/data/new/mix_sample_{i:.1f}_x2.npy'\n",
    "\n",
    "    cmd = f'python from_h5_to_npy.py {h5_path} {output_path} &'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0.npy SB_0.0_new \"Sensitivity: 0.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0.npy SB_1.0_new \"Sensitivity: 1.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0.npy SB_2.0_new \"Sensitivity: 2.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0.npy SB_3.0_new \"Sensitivity: 3.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0.npy SB_4.0_new \"Sensitivity: 4.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0.npy SB_5.0_new \"Sensitivity: 5.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0.npy SB_6.0_new \"Sensitivity: 6.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0.npy SB_7.0_new \"Sensitivity: 7.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0.npy SB_8.0_new \"Sensitivity: 8.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0.npy SB_9.0_new \"Sensitivity: 9.0\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0.npy SB_10.0_new \"Sensitivity: 10.0\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}.npy'\n",
    "    model_name = f'SB_{i:.1f}_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_aug_1.npy SB_0.0_aug_1_new \"Sensitivity: 0.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_aug_1.npy SB_1.0_aug_1_new \"Sensitivity: 1.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_aug_1.npy SB_2.0_aug_1_new \"Sensitivity: 2.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_aug_1.npy SB_3.0_aug_1_new \"Sensitivity: 3.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_aug_1.npy SB_4.0_aug_1_new \"Sensitivity: 4.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_aug_1.npy SB_5.0_aug_1_new \"Sensitivity: 5.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_aug_1.npy SB_6.0_aug_1_new \"Sensitivity: 6.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_aug_1.npy SB_7.0_aug_1_new \"Sensitivity: 7.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_aug_1.npy SB_8.0_aug_1_new \"Sensitivity: 8.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_aug_1.npy SB_9.0_aug_1_new \"Sensitivity: 9.0, Augmentation: 1\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_aug_1.npy SB_10.0_aug_1_new \"Sensitivity: 10.0, Augmentation: 1\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_aug_1.npy'\n",
    "    model_name = f'SB_{i:.1f}_aug_1_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Augmentation: 1'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_0.0_x2.npy SB_0.0_x2_new \"Sensitivity: 0.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_1.0_x2.npy SB_1.0_x2_new \"Sensitivity: 1.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_2.0_x2.npy SB_2.0_x2_new \"Sensitivity: 2.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_3.0_x2.npy SB_3.0_x2_new \"Sensitivity: 3.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_4.0_x2.npy SB_4.0_x2_new \"Sensitivity: 4.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_5.0_x2.npy SB_5.0_x2_new \"Sensitivity: 5.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_6.0_x2.npy SB_6.0_x2_new \"Sensitivity: 6.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_7.0_x2.npy SB_7.0_x2_new \"Sensitivity: 7.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_8.0_x2.npy SB_8.0_x2_new \"Sensitivity: 8.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_9.0_x2.npy SB_9.0_x2_new \"Sensitivity: 9.0, Luminosity: x2\"\n",
      "python train_CNN.py ../Sample/HVmodel/data/new/mix_sample_10.0_x2.npy SB_10.0_x2_new \"Sensitivity: 10.0, Luminosity: x2\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    train_file = f'../Sample/HVmodel/data/new/mix_sample_{i:.1f}_x2.npy'\n",
    "    model_name = f'SB_{i:.1f}_x2_new'\n",
    "    sample_type = f'Sensitivity: {i:.1f}, Luminosity: x2'\n",
    "    cmd = f'python train_CNN.py {train_file} {model_name} \"{sample_type}\"'\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
